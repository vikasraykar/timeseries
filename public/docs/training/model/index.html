<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  Single Layer Networks
  #


For simplicity for this chapter we will mainly introduce single layer networks for regression and classification.




  





stateDiagram-v2
    direction LR
    z1: $$x_1$$
    z2: $$x_2$$
    zi: $$x_i$$
    zM: $$x_d$$
    aj: $$a=\sum_i w_{i} x_i$$
    zj: $$z=h(a)$$
    z1 --&gt; aj:$$w_{1}$$
    z2 --&gt; aj:$$w_{2}$$
    zi --&gt; aj:$$w_{i}$$
    zM --&gt; aj:$$w_{d}$$
    aj --&gt; zj
    zj --&gt; END:::hidden
    note left of zM : Inputs
    note left of aj : Pre-activation
    note left of zj : Activation
    note left of END : Output
    classDef hidden display: none;



  Linear Regression
  #

Linear Regression is a single layer neural network for regression. The probability of 
  \(y\)

 for a given feature vector (
  \(\mathbf{x}\in \mathbb{R}^d\)

) is modelled as

  \[
\text{Pr}[y|\mathbf{x},\mathbf{w}] = \mathcal{N}(y|\mathbf{w}^T\mathbf{x},\sigma^2)
\]


where 
  \(\mathbf{w}\in \mathbb{R}^d\)

 are the weights/parameters of the model and 
  \(\mathcal{N}\)

 is the normal distribution with mean 
  \(\mathbf{w}^T\mathbf{x}\)

 and variance 
  \(\sigma^2\)

. The prediction is given by

  \[
\text{E}[y|\mathbf{x},\mathbf{w}] = \mathbf{w}^T\mathbf{x}
\]

">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/training/model/">
  <meta property="og:site_name" content="Deep Learning">
  <meta property="og:title" content="Models">
  <meta property="og:description" content="Single Layer Networks # For simplicity for this chapter we will mainly introduce single layer networks for regression and classification.
stateDiagram-v2 direction LR z1: $$x_1$$ z2: $$x_2$$ zi: $$x_i$$ zM: $$x_d$$ aj: $$a=\sum_i w_{i} x_i$$ zj: $$z=h(a)$$ z1 --&gt; aj:$$w_{1}$$ z2 --&gt; aj:$$w_{2}$$ zi --&gt; aj:$$w_{i}$$ zM --&gt; aj:$$w_{d}$$ aj --&gt; zj zj --&gt; END:::hidden note left of zM : Inputs note left of aj : Pre-activation note left of zj : Activation note left of END : Output classDef hidden display: none; Linear Regression # Linear Regression is a single layer neural network for regression. The probability of \(y\) for a given feature vector ( \(\mathbf{x}\in \mathbb{R}^d\) ) is modelled as \[ \text{Pr}[y|\mathbf{x},\mathbf{w}] = \mathcal{N}(y|\mathbf{w}^T\mathbf{x},\sigma^2) \] where \(\mathbf{w}\in \mathbb{R}^d\) are the weights/parameters of the model and \(\mathcal{N}\) is the normal distribution with mean \(\mathbf{w}^T\mathbf{x}\) and variance \(\sigma^2\) . The prediction is given by \[ \text{E}[y|\mathbf{x},\mathbf{w}] = \mathbf{w}^T\mathbf{x} \]">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
<title>Models | Deep Learning</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/docs/training/model/">
<link rel="stylesheet" href="/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css" integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.4cacd3980491d67e5f49ba6b4b58cd3873010a3c6c912cfe4c009ef436e9e73a.js" integrity="sha256-TKzTmASR1n5fSbprS1jNOHMBCjxskSz&#43;TACe9Dbp5zo=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>Deep Learning</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>















  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/supervised/" class="">Supervised learning</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/supervised/linear_regression/" class="">Linear Regression</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/training/" class="">Training deep neural networks</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/training/model/" class="active">Models</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/gradient_descent/" class="">Gradient Descent</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/backpropagation/" class="">Backpropagation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/normalization/" class="">Normalization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/regularization/" class="">Regularization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/training_loop/" class="">Training loop</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/activation_functions/" class="">Activation functions</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/quiz/" class="">Quiz</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/coding/" class="">Coding</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/transformers/" class="">Transformers</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/transformers101/" class="">Transformers</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/alignment/" class="">Alignment</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/rl/" class="">Reinforcement Learning</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/rl/basics/" class="">Basics</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>Models</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#single-layer-networks">Single Layer Networks</a>
      <ul>
        <li><a href="#linear-regression">Linear Regression</a></li>
        <li><a href="#logistic-regression">Logistic Regression</a></li>
      </ul>
    </li>
    <li><a href="#loss-functions">Loss functions</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h2 id="single-layer-networks">
  Single Layer Networks
  <a class="anchor" href="#single-layer-networks">#</a>
</h2>
<blockquote>
<p>For simplicity for this chapter we will mainly introduce single layer networks for regression and classification.</p></blockquote>


<script src="/mermaid.min.js"></script>

  <script>mermaid.initialize({
  "flowchart": {
    "useMaxWidth":true
  },
  "theme": "default"
}
)</script>




<pre class="mermaid">
stateDiagram-v2
    direction LR
    z1: $$x_1$$
    z2: $$x_2$$
    zi: $$x_i$$
    zM: $$x_d$$
    aj: $$a=\sum_i w_{i} x_i$$
    zj: $$z=h(a)$$
    z1 --> aj:$$w_{1}$$
    z2 --> aj:$$w_{2}$$
    zi --> aj:$$w_{i}$$
    zM --> aj:$$w_{d}$$
    aj --> zj
    zj --> END:::hidden
    note left of zM : Inputs
    note left of aj : Pre-activation
    note left of zj : Activation
    note left of END : Output
    classDef hidden display: none;
</pre>

<h3 id="linear-regression">
  Linear Regression
  <a class="anchor" href="#linear-regression">#</a>
</h3>
<p>Linear Regression is a single layer neural network for regression. The probability of <span>
  \(y\)
</span>
 for a given feature vector (<span>
  \(\mathbf{x}\in \mathbb{R}^d\)
</span>
) is modelled as
<span>
  \[
\text{Pr}[y|\mathbf{x},\mathbf{w}] = \mathcal{N}(y|\mathbf{w}^T\mathbf{x},\sigma^2)
\]
</span>

where <span>
  \(\mathbf{w}\in \mathbb{R}^d\)
</span>
 are the weights/<strong>parameters</strong> of the model and <span>
  \(\mathcal{N}\)
</span>
 is the <strong>normal</strong> distribution with mean <span>
  \(\mathbf{w}^T\mathbf{x}\)
</span>
 and variance <span>
  \(\sigma^2\)
</span>
. The prediction is given by
<span>
  \[
\text{E}[y|\mathbf{x},\mathbf{w}] = \mathbf{w}^T\mathbf{x}
\]
</span>
</p>
<blockquote class="book-hint warning">
<p>Without loss of generalization we ignore the bias term as it can be incorporated into the feature vector.</p>
</blockquote>
<p>Given a dataset <span>
  \(\mathcal{D}=\{\mathbf{x}_i \in \mathbb{R}^d,\mathbf{y}_i \in \mathbb{R}\}_{i=1}^N\)
</span>
 containing <span>
  \(n\)
</span>
 examples we need to estimate the parameter vector <span>
  \(\mathbf{w}\)
</span>
 by maximizing the likelihood of data.</p>
<blockquote>
<p>In practice we minimize the <strong>negative log likelihood</strong>.</p></blockquote>
<p>Let <span>
  \( \mu_i = \mathbf{w}^T\mathbf{x}_i\)
</span>
 be the model prediction for each example in the training dataset. The negative log likelihood (NLL) is given by
<span>
  \[
\begin{align}
L(\mathbf{w}) &= - \sum_{i=1}^{N} \log \left[\text{Pr}[y_i|\mathbf{x}_i,\mathbf{w}]\right] \nonumber \\
                       &= \frac{N}{2} \log(2\pi\sigma^2) + \frac{1}{2\sigma^2} \sum_{i=1}^{N} (y_i-\mu_i)^2 \nonumber \\

\end{align}
\]
</span>

This is equivalent to minimizing the <strong>Mean Squared Error</strong> (MSE) loss.
<span>
  \[
\begin{align}
L(\mathbf{w}) &= \frac{1}{N} \sum_{i=1}^{N} (y_i-\mu_i)^2 \nonumber \\
\end{align}
\]
</span>

We need to choose the model parameters that optimizes (minimizes) the loss function.
<span>
  \[
\hat{\mathbf{w}} = \argmin_{\mathbf{w}} L(\mathbf{w})
\]
</span>
</p>
<a  href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss"   target="_blank" rel="noopener"  class="book-btn">torch.nn.MSELoss</a>

<h3 id="logistic-regression">
  Logistic Regression
  <a class="anchor" href="#logistic-regression">#</a>
</h3>
<p>Logisitc Regression is a single layer neural network for binary classification. The probability of the positive class (<span>
  \(y=1\)
</span>
) for a given feature vector (<span>
  \(\mathbf{x}\in \mathbb{R}^d\)
</span>
) is given by
<span>
  \[
\text{Pr}[y=1|\mathbf{x},\mathbf{w}] = \sigma(\mathbf{w}^T\mathbf{x})
\]
</span>

where <span>
  \(\mathbf{w}\in \mathbb{R}^d\)
</span>
 are the weights/<strong>parameters</strong> of the model and <span>
  \(\sigma\)
</span>
 is the <strong>sigmoid</strong> activation function defined as
<span>
  \[
\sigma(x) = \frac{1}{1-e^{-z}}
\]
</span>
</p>
<blockquote class="book-hint warning">
<p>Without loss of generalization we ignore the bias term as it can be incorporated into the feature vector.</p>
</blockquote>
<p>Given a dataset <span>
  \(\mathcal{D}=\{\mathbf{x}_i \in \mathbb{R}^d,\mathbf{y}_i \in [0,1]\}_{i=1}^N\)
</span>
 containing <span>
  \(n\)
</span>
 examples we need to estimate the parameter vector <span>
  \(\mathbf{w}\)
</span>
 by maximizing the likelihood of data.</p>
<blockquote>
<p>In practice we minimize the <strong>negative log likelihood</strong>.</p></blockquote>
<p>Let <span>
  \( \mu_i = \text{Pr}[y_i=1|\mathbf{x}_i,\mathbf{w}] = \sigma(\mathbf{w}^T\mathbf{x}_i)\)
</span>
 be the model prediction for each example in the training dataset. The the negative log likelihood (NLL) is given by
<span>
  \[
\begin{align}
L(\mathbf{w}) &= - \sum_{i=1}^{N} \log\left[\mu_i^{y_i}(1-\mu_i)^{1-y_i}\right] \nonumber \\
                       &= - \sum_{i=1}^{N} \left[ y_i\log(\mu_i) + (1-y_i)\log(1-\mu_i) \right] \nonumber \\

\end{align}
\]
</span>

This is referred to as the <strong>Binary Cross Entropy</strong> (BCE) loss. We need to choose the model parameters that optimizes (minimizes) the loss function.
<span>
  \[
\hat{\mathbf{w}} = \argmin_{\mathbf{w}} L(\mathbf{w})
\]
</span>
</p>
<p><a  href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss"   target="_blank" rel="noopener"  class="book-btn">torch.nn.BCELoss</a>
 <a  href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss"   target="_blank" rel="noopener"  class="book-btn">torch.nn.BCEWithLogitsLoss</a>
</p>
<details ><summary>Entropy</summary>
  <div class="markdown-inner">
<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \(\)
</span>
The entropy of a discrete random variable <span>
  \(X\)
</span>
 with <span>
  \(K\)
</span>
 states/categories with distribution <span>
  \(p_k = \text{Pr}(X=k)\)
</span>
 for <span>
  \(k=1,...,K\)
</span>
  is a measure of uncertainty and is defined as follows.
<span>
  \[H(X) = \sum_{k=1}^{K} p_k \log_2 \frac{1}{p_k} = - \sum_{k=1}^{K} p_k \log_2 p_k \]
</span>
<span>
  \(\)
</span>
The term <span>
  \(\log_2\frac{1}{p}\)
</span>
 quantifies the notion or surprise or uncertainty and entropy is the average uncertainty. The unit is bits (<span>
  \(\in [0,\log_2 K]\)
</span>
) (or nats incase of natural log). The discrete distribution with maximum entropy (<span>
  \(\log_2 K\)
</span>
) is uniform. The discrete distribution with minimum entropy (<span>
  \(0\)
</span>
) is any delta function which puts all mass on one state/category.
<p>Binary entropy</p>
<span>
  \(\)
</span>
For a binary random variable <span>
  \(X \in {0,1}\)
</span>
 with <span>
  \(\text{Pr}(X=1) = \theta\)
</span>
 and <span>
  \(\text{Pr}(X=0) = 1-\theta\)
</span>
 the entropy is as follows.
<span>
  \[H(\theta) = - [ \theta \log_2 \theta + (1-\theta) \log_2 (1-\theta) ] \]
</span>
<span>
  \[H(\theta) \in [0,1]\]
</span>
 and is maximum when <span>
  \(\theta=0.5\)
</span>
.
<p>Cross entropy</p>
<span>
  \(\)
</span>
Cross entropy is the average number of bits needed to encode the data from from a source <span>
  \(p\)
</span>
 when we model it using <span>
  \(q\)
</span>
.
<span>
  \[H(p,q) = - \sum_{k=1}^{K} p_k \log_2 q_k \]
</span>
  </div>
</details>
<h2 id="loss-functions">
  Loss functions
  <a class="anchor" href="#loss-functions">#</a>
</h2>
<p><a href="https://pytorch.org/docs/stable/nn.html#loss-functions">https://pytorch.org/docs/stable/nn.html#loss-functions</a></p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#single-layer-networks">Single Layer Networks</a>
      <ul>
        <li><a href="#linear-regression">Linear Regression</a></li>
        <li><a href="#logistic-regression">Logistic Regression</a></li>
      </ul>
    </li>
    <li><a href="#loss-functions">Loss functions</a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












