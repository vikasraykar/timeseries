[{"id":0,"href":"/docs/training/","title":"Training deep neural networks","section":"Docs","content":"The goal of training is to find the value of the parameters of a model to make effective predictions.\nWe choose the model parameters by optimizing a loss funcntion.\n"},{"id":1,"href":"/docs/training/model/","title":"Models","section":"Training deep neural networks","content":" Models # Parameters # Loss functions # "},{"id":2,"href":"/docs/training/gradient_descent/","title":"Gradient Descent","section":"Training deep neural networks","content":" Gradient Descent # Steepest descent.\nLet \\(\\mathbf{w}\\) be a vector of all the parameters for a model.\nLet \\(L(\\mathbf{w})\\) be the loss function (or error function).\nWe need to choose the model parameters that optimizes (minimizes) the loss function.\n\\[ \\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}} L(\\mathbf{w}) \\] Let \\(\\nabla L(\\mathbf{w})\\) be the gradient vector, where each element is the partial derivative of the loss fucntion wrt each parameter.\nThe gradient vector points in the direction of the greatest rate of increase of the loss function.\nSo to mimimize the loss function we take small steps in the direction of \\(-\\nabla L(\\mathbf{w})\\) .\nAt the mimimum \\(\\nabla L(\\mathbf{w})=0\\) .\n\\(\\nabla L(\\mathbf{w})=0\\) .\nStationary points \\(\\nabla L(\\mathbf{w})=0\\) are knows as stationary points, which can be either a minima, maxima or a saddle point. The necessary and sufficient condition for a local minima is The gradient of the loss function should be zero. The Hessian matrix should be positive definite. For now we will assume the gradient is given. For deep neural networks the gradient can be computed efficiently via backpropagation (which we will revisit later).\nBatch Gradient Descent # We take a small step in the direction of the negative gradient.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L(\\mathbf{w}^{t-1}) \\] The parameter \\(\\eta \u003e 0\\) is called the learning rate and determines the step size at each iteration.\nThis update is repeated multiple times (till covergence).\nfor epoch in range(n_epochs): dw = gradient(loss, data, w) w = w - lr * dw Each step requires that the entire training data be processed to compute the gradient \\(\\nabla L(\\mathbf{w}^{t-1})\\) . For large datasets this is not comptationally efficient.\nStochastic Gradient Descent # In general most loss functions can be written as sum over each training instance. \\[ L(\\mathbf{w}) = \\sum_{i=1}^{N} L_i(\\mathbf{w}) \\] In Stochastic Gradient Descent (SGD) we update the parameters one data point at a time. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L_i(\\mathbf{w}^{t-1}) \\] A complete passthrough of the whole dataset is called an epoch. Training is done for multiple epochs depending on the size of the dataset.\nfor epoch in range(n_epochs): for i in range(n_data): dw = gradient(loss, data[i], w) w = w - lr * dw SGD is much faster and more computationally efficient, but it has noise in the estimation of the gradient. Since it updates the weight frequently, it can lead to big oscillations and that makes the training process highly unstable. Bottou, L. (2010). Large-Scale Machine Learning with Stochastic Gradient Descent. In: Lechevallier, Y., Saporta, G. (eds) Proceedings of COMPSTAT'2010. Physica-Verlag HD.\nMini-batch Stochastic Gradient Descent # Using a single example results in a very noisy estimate of the gradient. So we use a small random subset of data called mini-batch of size B (batch size) to compute the gradient.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L_{batch}(\\mathbf{w}^{t-1}) \\] for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) w = w - lr * dw PyTorch optimizer = optim.SGD(model.parameters(), lr=1e-3) Mini-batch SGD is the most commonly used method and is sometimes refered to as just SGD.\nTypical choices of the batch size are B=32,64,128,256,.. In practice we do a random shuffle of the data per epoch. In practice, mini-batch SGD is the most frequently used variation because it is both computationally cheap and results in more robust convergence.\nAdding momentum # One of the basic improvements over SGD comes from adding a momentum term.\nAt every time step, we update velocity by decaying the previous velocity by a factor of \\(0 \\leq \\mu \\leq 1\\) (called the momentum parameter) and adding the current gradient update. \\[ \\mathbf{v}^{t-1} \\leftarrow \\mu \\mathbf{v}^{t-2} - \\eta \\nabla L(\\mathbf{w}^{t-1}) \\] Then, we update our weights in the direction of the velocity vector. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} + \\mathbf{v}^{t-1} \\] for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient v = momentum * v - lr * dw # velocity w = w + v PyTorch optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) We now have two hyper-parameters learnign rate and momentum. Typically we set the momentum parameter to 0.9.\nEffective learning rate \\(\\) One interpretation of momentum to increase the effective learning rate from \\(\\eta\\) to \\(\\frac{\\eta}{(1-\\mu)}\\) . If we make the approximation that the gradient is unchanging then \\[ -\\eta \\nabla L \\{1+\\mu+\\mu^2+...\\} = - \\frac{\\eta}{1-\\mu} \\nabla L \\] By contrast, in a region of high curvature in which gradient descent is oscillatory, successive contributions from the momentum term will tend to cancel and effective learning rate will be close to \\(\\eta\\) . We can now escape local minima or saddle points because we keep moving downwards even though the gradient of the mini-batch might be zero. Momentum can also help us reduce the oscillation of the gradients because the velocity vectors can smooth out these highly changing landscapes. It reduces the noise of the gradients and follows a more direct walk down the landscape. Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. 2013. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28 (ICML'13). JMLR.org, III–1139–III–1147.\nAdaptive Learning Rates # Adagrad # RMSProp # Adam # Learning rate schedule # Learning curve # Training loop # "},{"id":3,"href":"/docs/training/normalization/","title":"Normalization","section":"Training deep neural networks","content":" Data normalization # Batch normalization # Layer normalization # "},{"id":4,"href":"/docs/training/backpropagation/","title":"Backpropagation","section":"Training deep neural networks","content":" Backpropagation # Algorithmic differenciation # Autograd # "},{"id":5,"href":"/docs/training/quiz/","title":"Quiz","section":"Training deep neural networks","content":" Question 1 # "},{"id":6,"href":"/docs/training/coding/","title":"Coding","section":"Training deep neural networks","content":" Coding assignment # "},{"id":7,"href":"/docs/shortcodes/buttons/","title":"Buttons","section":"Shortcodes","content":" Buttons # Buttons are styled links that can lead to local page or external link.\nExample # {{\u0026lt; button relref=\u0026#34;/\u0026#34; [class=\u0026#34;...\u0026#34;] \u0026gt;}}Get Home{{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026#34;https://github.com/alex-shpak/hugo-book\u0026#34; \u0026gt;}}Contribute{{\u0026lt; /button \u0026gt;}} Get Home Contribute "},{"id":8,"href":"/docs/shortcodes/columns/","title":"Columns","section":"Shortcodes","content":" Columns # Columns help organize shorter pieces of content horizontally for readability.\nExample # {{% columns [ratio=\u0026#34;1:1\u0026#34;] [class=\u0026#34;...\u0026#34;] %}} \u0026lt;!-- begin columns block --\u0026gt; # Left Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Mid Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Right Content Lorem markdownum insigne... {{% /columns %}} Left Content # Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nMid Content # Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter!\nRight Content # Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nSettings size ratio for columns # {{% columns ratio=\u0026#34;1:2\u0026#34; %}} \u0026lt;!-- begin columns block --\u0026gt; ## x1 Column Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; ## x2 Column Lorem markdownum insigne... {{% /columns %}} x1 Column # Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nx2 Column # Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter!\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\n"},{"id":9,"href":"/docs/shortcodes/mermaid/","title":"Mermaid","section":"Shortcodes","content":" Mermaid Chart # MermaidJS is library for generating svg charts and diagrams from text.\nOverride Mermaid initialization config\nTo override the initialization config for Mermaid, create a mermaid.json file in your assets folder!\nExample # {{\u0026lt; mermaid [class=\u0026#34;...\u0026#34;] \u0026gt;}} stateDiagram-v2 State1: The state with a note note right of State1 Important information! You can write notes. end note State1 --\u0026gt; State2 note left of State2 : This is the note to the left. {{\u0026lt; /mermaid \u0026gt;}} stateDiagram-v2 State1: The state with a note note right of State1 Important information! You can write notes. end note State1 --\u003e State2 note left of State2 : This is the note to the left. "},{"id":10,"href":"/docs/shortcodes/section/","title":"Section","section":"Shortcodes","content":" Section # Section renders pages in section as definition list, using title and description. Optional param summary can be used to show or hide page summary\nExample # {{\u0026lt; section [summary] \u0026gt;}} First Page First page # Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nSecond Page Second Page # Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n"},{"id":11,"href":"/docs/shortcodes/section/first-page/","title":"First Page","section":"Section","content":" First page # Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n"},{"id":12,"href":"/docs/shortcodes/section/second-page/","title":"Second Page","section":"Section","content":" Second Page # Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n"},{"id":13,"href":"/docs/shortcodes/tabs/","title":"Tabs","section":"Shortcodes","content":" Tabs # Tabs let you organize content by context, for example installation instructions for each supported platform.\n{{% tabs \u0026#34;id\u0026#34; %}} {{% tab \u0026#34;MacOS\u0026#34; %}} # MacOS Content {{% /tab %}} {{% tab \u0026#34;Linux\u0026#34; %}} # Linux Content {{% /tab %}} {{% tab \u0026#34;Windows\u0026#34; %}} # Windows Content {{% /tab %}} {{% /tabs %}} Example # MacOS MacOS # This is tab MacOS content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nLinux Linux # This is tab Linux content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nWindows Windows # This is tab Windows content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\n"}]