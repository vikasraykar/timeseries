[{"id":0,"href":"/docs/rl/basics/","title":"Basics","section":"Reinforcement Learning","content":" Reinforcement learning basics # slides Reinforcement Learning (RL) is a natural computational paradigm for agents learning from interaction to achieve a goal. Deep learning (DL) provides a powerful general-purpose representation learning framework. A combination of these two has recently emerged as a strong contender for artificial general intelligence. This tutorial will provide a gentle exposition of RL concepts and DL based RL with a focus on policy gradients.\n\\(\\) The agent-environment interaction # The reinforcement learning (RL) framework is an abstraction of the problem of goal directed learning from interaction.\nThe learner and the decision maker is called the agent. The thing it interacts with (everything outside the agent) is called the environment. The agent and the environment interact continually. In the RL framework any problem of learning goal-directed behavior is abstracted to three signals passing back and forth between the agent and the environment.\none signal to represent the choices made by the agent (the actions) one signal to represent the basis on which the choices are made (the states) one signal to define the agents goal (rewards) In general, actions can be any decisions we want to learn how to make, and the states can be anything we can know that might be useful in making them. More formally at each time step \\(t\\) The agent receives a representation of the environments state, \\(S_t \\in \\mathcal{S}\\) , where \\(\\mathcal{S}\\) is the set of possible states. On the basis of \\(S_t\\) the agent selects an action \\(A_t \\in \\mathcal{A}(S_t)\\) , where \\(\\mathcal{A}(S_t)\\) the set of actions available in state \\(S_t\\) . One time step later, as a result of the action \\(A_t\\) the agent receives a scalar reward, \\(R_{t+1} \\in \\mathcal{R} \\subset \\mathbb{R}\\) . The agent then observes a new state \\(S_{t+1}\\) . Policy # At each time step, the agent essentially has to implement a mapping (called the agents policy) from states to actions.\nThe agents (stochastic) policy is denoted by \\(\\pi_t\\) , where\n\\[ \\pi_t(a|s) = \\mathbb{P}[A_t=a|S_t=s]. \\] Markov Decision Processes # In the most general case the environment response may depend on everything that has happened earlier. \\[ Pr\\left\\{S_{t+1}=s^{'},R_{t+1}=r|S_0,A_0,R_1,...,S_{t-1},A_{t-1},R_t,S_t,A_t,\\right\\}. \\] If the state signal has Markov property then the response depends only on the state and action representations at \\(t\\) \\[ Pr\\left\\{S_{t+1}=s^{'},R_{t+1}=r|S_t,A_t,\\right\\}. \\] A RL task that satisfies the Markov property is called a Markov Decision Process (MDP).\nGoals and rewards # The agents goal is to maximize the total amount of cumulative reward it receives over the long run (and not the immediate reward). If we want the agent to achieve some goal, we must specify the rewards to in in such a way that in maximizing them the agent will also achieve our goals.\nEpisodic tasks have a natural notion of the final time step. The agent-environment interaction naturally breaks into sub sequences, which are called episodes, such as plays of a game, trips through a maze, etc. Each episode ends in a special state called the terminal state, followed by a reset to the standard starting state.\nFor continuing tasks the agent-environment interaction goes on continually without limit.\nLet \\(R\\_{t+1},R\\_{t+2},R\\_{t+3},...\\) be the sequence of rewards received after time step \\(t\\) .\nThe return \\(G_t\\) is a defined as some specific function (for example, the sum) of the reward sequence. For episodic tasks \\[ G_t = R_{t+1}+R_{t+2}+R_{t+3}+...+R_{T}, \\] where \\(T\\) is the final time step.\nFor continuing tasks we need an additional concept of discounting. The discounted return is given by \\[ G_t = R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+... = \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1} = R_{t+1}+\\gamma G_{t+1}, \\] where \\(0 \\leq \\gamma \\leq 1\\) is a parameter called the discount rate.\nThe discount rate determines the present value of future rewards. A reward received \\(k\\) time steps in the future is worth only \\(\\gamma^{k-1}\\) times what it would have been worth if it were received immediately.\nThe discount rate determines the present value of future rewards.\nAs \\(\\gamma\\) approaches 1 the agent becomes more farsighted.\nAs \\(\\gamma\\) approaches 0 the agent becomes more myopic.\nThe agents goal is to choose the actions to maximize the expected discounted return.\nValue Functions # A value function is a prediction of future reward. Recall that, the agents (stochastic) policy is denoted by \\(\\pi\\) , where \\[ \\pi(a|s) = \\mathbb{P}[A_t=a|S_t=s]. \\] The value of state \\(s\\) under a policy \\(\\pi\\) is the expected return when starting in \\(s\\) and following the policy \\(\\pi\\) thereafter. \\[ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t=s] = \\mathbb{E}_{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}|S_t=s]. \\] \\(v_{\\pi}\\) is called the state-value function for policy \\(\\pi\\) .\nHow much reward will I get from state \\(s\\) under policy \\(\\pi\\) ?\nThe value of action \\(a\\) in state \\(s\\) under a policy \\(\\pi\\) is the expected return starting in \\(s\\) , taking the action \\(a\\) , and thereafter following policy \\(\\pi\\) .\n\\[ q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a] =\\mathbb{E}_{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}|S_t=s,A_t=a]. \\] \\(q_{\\pi}\\) is called the action-value function or the Q-value function for policy \\(\\pi\\) .\nHow much reward will I get from action \\(a\\) in state \\(s\\) under policy \\(\\pi\\) ?\nValue functions decompose into a Bellman equation which specifies the relation between the value of \\(s\\) and the value of its possible successor states.\n\\[ v_{\\pi}(s) = \\mathbb{E}[r+\\gamma v_{\\pi}(s^{'})] \\] Optimal Value Functions # The agents goal is to find a policy to maximize the expected discounted return.\nWhy are we talking about value functions ?\nValue functions define a partial ordering over polices.\n\\[ \\pi \\geq \\pi{'} \\text{ if and only if } v_{\\pi}(s) \\geq v_{\\pi}(s^{'}) \\text{ for all } s \\in \\mathcal{S} \\] The optimal policy is the one which has the maximum state-value function.\noptimal state-value function\n\\[ v_{*}(s) = \\max_{\\pi} v_{\\pi}(s) \\text{ for all } s \\in \\mathcal{S} \\] Bellman\u0026rsquo;s optimality equation\n\\[ v_{*}(s) = \\max_{a \\in \\mathcal{A}(s)} \\sum_{s^{'},r} p(s^{'},r|s,a)[r+\\gamma v_{*}(s^{'}) ] \\] optimal action-value function\n\\[ q_{*}(s,a) = \\max_{\\pi} q_{\\pi}(s,a) \\text{ for all } s \\in \\mathcal{S} \\text{ and } a \\in \\mathcal{A} \\] Bellman\u0026rsquo;s optimality equation\n\\[ q_{*}(s,a) = \\sum_{s^{'},r} p(s^{'},r|s,a)[r+\\gamma \\max_{a^{'}} q_{*}(s^{'},a^{'}) ] \\] The value of the start state must equal the discounted value of the expected next state plus the reward expected along the way.\nPolicy gradient methods # The goal is to learn a parametrized policy that can select actions without consulting a value function. Note that a value function will still be used to learn the policy parameter, but is not required for action selection.\nLet \\(\\theta \\in \\mathbb{R}^{d}\\) represent the policy\u0026rsquo;s parameter vector. The parameterized policy is written as\n\\[ \\pi(a|s,\\theta) = \\text{Pr}[A_t=a|S_t=s,\\theta_t=\\theta] \\] This is the probability that action \\(a\\) is taken at time \\(t\\) given that the agent is in state \\(s\\) at time \\(t\\) with parameter \\(\\theta\\) .\nWe will estimate the policy parameter \\(\\theta\\) to maximize a performance measure \\(J(\\theta)\\) .\n\\[ \\widehat{\\theta} = \\arg \\max_{\\theta} J(\\theta) \\] As usual we will use stochastic gradient ascent\n\\[ \\theta_{t+1} = \\theta_(t) + \\alpha \\widehat{\\nabla J(\\theta_t)}, \\] where \\(\\widehat{\\nabla J(\\theta_t)}\\) is a stochastic estimate of the gradient whose expectation approximates the true gradient.\nFor the episodic case the performance is defined as the value of the start state under the parameterized policy.\n\\[ J(\\theta) = v_{\\pi_{\\theta}}(s_0) \\] Recall, the value of a state \\(s\\) under a policy \\(\\pi\\) is the expected return when starting in \\(s\\) and following the policy \\(\\pi\\) thereafter.\n\\[ v_{\\pi_\\theta}(s_0) = \\mathbb{E}_{\\pi_\\theta}[G_t|S_t=s_0] = \\mathbb{E}_{\\pi_\\theta}[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}|S_t=s_0] \\] Log-Derivative trick If \\(x \\sim p\\_{\\theta}(.)\\) then \\[ \\nabla_\\theta \\mathbb{E}[f(x)] = \\nabla_\\theta \\int p_{\\theta}(x) f(x) dx = \\int \\frac{p_{\\theta}(x)}{p_{\\theta}(x)} \\nabla_\\theta p_{\\theta}(x) f(x) dx \\] \\[ \\nabla_\\theta \\mathbb{E}[f(x)] = \\int p_{\\theta}(x) \\nabla_\\theta \\log p_{\\theta}(x) f(x) dx = \\mathbb{E}[f(x)\\nabla_\\theta \\log p_{\\theta}(x)] \\] We also need the compute the gradient of the log probability of an episode\nGradient of the log probability of an episode\nLet \\(\\tau\\) be an episode of length \\(T\\) defined as \\[ \\tau=(s_0,a_0,r_1,s_1,a_1,r_2,....,a_{T-1},r_T,s_T). \\] Then \\[ \\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\nabla_{\\theta} \\log \\left(\\mu(s_0) \\prod_{t=0}^{T-1} \\pi_{\\theta}(a_t|s_t)\\text{Pr}(s_{t+1}|s_t,a_t) \\right) \\] \\[ \\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\nabla_{\\theta} \\left[\\log \\mu(s_0) + \\sum_{t=0}^{T-1} ( \\log \\pi_{\\theta}(a_t|s_t)+ \\log \\text{Pr}(s_{t+1}|s_t,a_t) ) \\right] \\] \\[ \\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\nabla_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t|s_t) \\] Observe that when taking gradients, the state dynamics disappear!\nUsing the above two tricks\n\\[ \\nabla_{\\theta} v_{\\pi_\\theta}(s_0) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[ G_{\\tau}\\nabla_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t|s_t)\\right] \\] References # http://karpathy.github.io/2016/05/31/rl/ https://www.davidsilver.uk/teaching/ http://incompleteideas.net/sutton/book/the-book-2nd.html http://rll.berkeley.edu/deeprlcourse/ https://gym.openai.com/ https://deepmind.com/blog/deep-reinforcement-learning/ http://www.scholarpedia.org/article/Policy_gradient_methods#Assumptions_and_Notation https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/ http://www.1-4-5.net/~dmm/ml/log_derivative_trick.pdf "},{"id":1,"href":"/docs/supervised/","title":"Supervised learning","section":"Docs","content":"Some core supervised learning algorithms.\n"},{"id":2,"href":"/docs/training/","title":"Training deep neural networks","section":"Docs","content":"The goal of training is to find the value of the parameters of a model to make effective predictions.\nWe choose the model parameters by optimizing a loss funcntion.\n"},{"id":3,"href":"/docs/transformers/transformers101/","title":"Transformers","section":"Transformers","content":" Alignment # https://github.com/vikasraykar/wiki/wiki/Transformers-101\n"},{"id":4,"href":"/docs/transformers/alignment/","title":"Alignment","section":"Transformers","content":" Alignment # LLMs are typically trained for next-token prediction.\nPre-trained LLMs may not be able to follow user instructions because they were not trained to do so.\nPre-trained LLMs may generate harmful content or perpetuate biases inherent in their training data.\n--- title: LLM training stages --- flowchart LR subgraph Pre-training A[Pre-training] end subgraph Post-training B[\"Instruction Alignment (SFT)\"] C[\"Preference Alignment (RLHF)\"] end subgraph Inference D[Prompt engineering] end A--\u003eB B--\u003eC C--\u003eD Fine tune LLMs with labelled data # Supervised Fine Tuning (SFT) # Training data is task specific instructions paired with their expected outputs.\nDuring backward pass we the force the loss corresponding to the instruction to be zero.\nParameter Efficient Fine Tuning (PEFT) # LoRA # QLoRA # Soft prompts # Fine tune LLMs with reward models # Alignment during inference # Prompting\n"},{"id":5,"href":"/docs/training/model/","title":"Models","section":"Training deep neural networks","content":" Single Layer Networks # For simplicity for this chapter we will mainly introduce single layer networks for regression and classification.\nstateDiagram-v2 direction LR z1: $$x_1$$ z2: $$x_2$$ zi: $$x_i$$ zM: $$x_d$$ aj: $$a=\\sum_i w_{i} x_i$$ zj: $$z=h(a)$$ z1 --\u003e aj:$$w_{1}$$ z2 --\u003e aj:$$w_{2}$$ zi --\u003e aj:$$w_{i}$$ zM --\u003e aj:$$w_{d}$$ aj --\u003e zj zj --\u003e END:::hidden note left of zM : Inputs note left of aj : Pre-activation note left of zj : Activation note left of END : Output classDef hidden display: none; Linear Regression # Linear Regression is a single layer neural network for regression. The probability of \\(y\\) for a given feature vector ( \\(\\mathbf{x}\\in \\mathbb{R}^d\\) ) is modelled as \\[ \\text{Pr}[y|\\mathbf{x},\\mathbf{w}] = \\mathcal{N}(y|\\mathbf{w}^T\\mathbf{x},\\sigma^2) \\] where \\(\\mathbf{w}\\in \\mathbb{R}^d\\) are the weights/parameters of the model and \\(\\mathcal{N}\\) is the normal distribution with mean \\(\\mathbf{w}^T\\mathbf{x}\\) and variance \\(\\sigma^2\\) . The prediction is given by \\[ \\text{E}[y|\\mathbf{x},\\mathbf{w}] = \\mathbf{w}^T\\mathbf{x} \\] Without loss of generalization we ignore the bias term as it can be incorporated into the feature vector.\nGiven a dataset \\(\\mathcal{D}=\\{\\mathbf{x}_i \\in \\mathbb{R}^d,\\mathbf{y}_i \\in \\mathbb{R}\\}_{i=1}^N\\) containing \\(n\\) examples we need to estimate the parameter vector \\(\\mathbf{w}\\) by maximizing the likelihood of data.\nIn practice we minimize the negative log likelihood.\nLet \\( \\mu_i = \\mathbf{w}^T\\mathbf{x}_i\\) be the model prediction for each example in the training dataset. The negative log likelihood (NLL) is given by \\[ \\begin{align} L(\\mathbf{w}) \u0026= - \\sum_{i=1}^{N} \\log \\left[\\text{Pr}[y_i|\\mathbf{x}_i,\\mathbf{w}]\\right] \\nonumber \\\\ \u0026= \\frac{N}{2} \\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i-\\mu_i)^2 \\nonumber \\\\ \\end{align} \\] This is equivalent to minimizing the Mean Squared Error (MSE) loss. \\[ \\begin{align} L(\\mathbf{w}) \u0026= \\frac{1}{N} \\sum_{i=1}^{N} (y_i-\\mu_i)^2 \\nonumber \\\\ \\end{align} \\] We need to choose the model parameters that optimizes (minimizes) the loss function. \\[ \\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}} L(\\mathbf{w}) \\] torch.nn.MSELoss Logistic Regression # Logisitc Regression is a single layer neural network for binary classification. The probability of the positive class ( \\(y=1\\) ) for a given feature vector ( \\(\\mathbf{x}\\in \\mathbb{R}^d\\) ) is given by \\[ \\text{Pr}[y=1|\\mathbf{x},\\mathbf{w}] = \\sigma(\\mathbf{w}^T\\mathbf{x}) \\] where \\(\\mathbf{w}\\in \\mathbb{R}^d\\) are the weights/parameters of the model and \\(\\sigma\\) is the sigmoid activation function defined as \\[ \\sigma(x) = \\frac{1}{1-e^{-z}} \\] Without loss of generalization we ignore the bias term as it can be incorporated into the feature vector.\nGiven a dataset \\(\\mathcal{D}=\\{\\mathbf{x}_i \\in \\mathbb{R}^d,\\mathbf{y}_i \\in [0,1]\\}_{i=1}^N\\) containing \\(n\\) examples we need to estimate the parameter vector \\(\\mathbf{w}\\) by maximizing the likelihood of data.\nIn practice we minimize the negative log likelihood.\nLet \\( \\mu_i = \\text{Pr}[y_i=1|\\mathbf{x}_i,\\mathbf{w}] = \\sigma(\\mathbf{w}^T\\mathbf{x}_i)\\) be the model prediction for each example in the training dataset. The the negative log likelihood (NLL) is given by \\[ \\begin{align} L(\\mathbf{w}) \u0026= - \\sum_{i=1}^{N} \\log\\left[\\mu_i^{y_i}(1-\\mu_i)^{1-y_i}\\right] \\nonumber \\\\ \u0026= - \\sum_{i=1}^{N} \\left[ y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i) \\right] \\nonumber \\\\ \\end{align} \\] This is referred to as the Binary Cross Entropy (BCE) loss. We need to choose the model parameters that optimizes (minimizes) the loss function. \\[ \\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}} L(\\mathbf{w}) \\] torch.nn.BCELoss torch.nn.BCEWithLogitsLoss Entropy \\(\\) The entropy of a discrete random variable \\(X\\) with \\(K\\) states/categories with distribution \\(p_k = \\text{Pr}(X=k)\\) for \\(k=1,...,K\\) is a measure of uncertainty and is defined as follows. \\[H(X) = \\sum_{k=1}^{K} p_k \\log_2 \\frac{1}{p_k} = - \\sum_{k=1}^{K} p_k \\log_2 p_k \\] \\(\\) The term \\(\\log_2\\frac{1}{p}\\) quantifies the notion or surprise or uncertainty and entropy is the average uncertainty. The unit is bits ( \\(\\in [0,\\log_2 K]\\) ) (or nats incase of natural log). The discrete distribution with maximum entropy ( \\(\\log_2 K\\) ) is uniform. The discrete distribution with minimum entropy ( \\(0\\) ) is any delta function which puts all mass on one state/category. Binary entropy\n\\(\\) For a binary random variable \\(X \\in {0,1}\\) with \\(\\text{Pr}(X=1) = \\theta\\) and \\(\\text{Pr}(X=0) = 1-\\theta\\) the entropy is as follows. \\[H(\\theta) = - [ \\theta \\log_2 \\theta + (1-\\theta) \\log_2 (1-\\theta) ] \\] \\[H(\\theta) \\in [0,1]\\] and is maximum when \\(\\theta=0.5\\) . Cross entropy\n\\(\\) Cross entropy is the average number of bits needed to encode the data from from a source \\(p\\) when we model it using \\(q\\) . \\[H(p,q) = - \\sum_{k=1}^{K} p_k \\log_2 q_k \\] Loss functions # https://pytorch.org/docs/stable/nn.html#loss-functions\n"},{"id":6,"href":"/docs/transformers/","title":"Transformers","section":"Docs","content":"Transformer architecture deep dive.\n"},{"id":7,"href":"/docs/training/gradient_descent/","title":"Gradient Descent","section":"Training deep neural networks","content":" Gradient Descent # Steepest descent.\nLet \\(\\mathbf{w}\\) be a vector of all the parameters for a model.\nLet \\(L(\\mathbf{w})\\) be the loss function (or error function).\nWe need to choose the model parameters that optimizes (minimizes) the loss function.\n\\[ \\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}} L(\\mathbf{w}) \\] Let \\(\\nabla L(\\mathbf{w})\\) be the gradient vector, where each element is the partial derivative of the loss fucntion wrt each parameter.\nThe gradient vector points in the direction of the greatest rate of increase of the loss function.\nSo to mimimize the loss function we take small steps in the direction of \\(-\\nabla L(\\mathbf{w})\\) .\nAt the mimimum \\(\\nabla L(\\mathbf{w})=0\\) .\n\\(\\nabla L(\\mathbf{w})=0\\) .\nStationary points \\(\\nabla L(\\mathbf{w})=0\\) are knows as stationary points, which can be either a minima, maxima or a saddle point. The necessary and sufficient condition for a local minima is The gradient of the loss function should be zero. The Hessian matrix should be positive definite. For now we will assume the gradient is given. For deep neural networks the gradient can be computed efficiently via backpropagation (which we will revisit later).\nBatch Gradient Descent # We take a small step in the direction of the negative gradient.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L(\\mathbf{w}^{t-1}) \\] The parameter \\(\\eta \u003e 0\\) is called the learning rate and determines the step size at each iteration.\nThis update is repeated multiple times (till covergence).\nfor epoch in range(n_epochs): dw = gradient(loss, data, w) w = w - lr * dw Each step requires that the entire training data be processed to compute the gradient \\(\\nabla L(\\mathbf{w}^{t-1})\\) . For large datasets this is not comptationally efficient.\nStochastic Gradient Descent # In general most loss functions can be written as sum over each training instance. \\[ L(\\mathbf{w}) = \\sum_{i=1}^{N} L_i(\\mathbf{w}) \\] In Stochastic Gradient Descent (SGD) we update the parameters one data point at a time. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L_i(\\mathbf{w}^{t-1}) \\] A complete passthrough of the whole dataset is called an epoch. Training is done for multiple epochs depending on the size of the dataset.\nfor epoch in range(n_epochs): for i in range(n_data): dw = gradient(loss, data[i], w) w = w - lr * dw SGD is much faster and more computationally efficient, but it has noise in the estimation of the gradient. Since it updates the weight frequently, it can lead to big oscillations and that makes the training process highly unstable. Bottou, L. (2010). Large-Scale Machine Learning with Stochastic Gradient Descent. In: Lechevallier, Y., Saporta, G. (eds) Proceedings of COMPSTAT'2010. Physica-Verlag HD.\nMini-batch Stochastic Gradient Descent # Using a single example results in a very noisy estimate of the gradient. So we use a small random subset of data called mini-batch of size B (batch size) to compute the gradient.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L_{batch}(\\mathbf{w}^{t-1}) \\] for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) w = w - lr * dw PyTorch optimizer = optim.SGD(model.parameters(), lr=1e-3) Mini-batch SGD is the most commonly used method and is sometimes refered to as just SGD.\nTypical choices of the batch size are B=32,64,128,256,.. In practice we do a random shuffle of the data per epoch. In practice, mini-batch SGD is the most frequently used variation because it is both computationally cheap and results in more robust convergence.\nAdding momentum # One of the basic improvements over SGD comes from adding a momentum term.\nAt every time step, we update velocity by decaying the previous velocity by a factor of \\(0 \\leq \\mu \\leq 1\\) (called the momentum parameter) and adding the current gradient update. \\[ \\mathbf{v}^{t-1} \\leftarrow \\mu \\mathbf{v}^{t-2} - \\eta \\nabla L(\\mathbf{w}^{t-1}) \\] Then, we update our weights in the direction of the velocity vector. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} + \\mathbf{v}^{t-1} \\] for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient v = momentum * v - lr * dw # velocity w = w + v PyTorch optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) We now have two hyper-parameters learnign rate and momentum. Typically we set the momentum parameter to 0.9.\nEffective learning rate \\(\\) One interpretation of momentum to increase the effective learning rate from \\(\\eta\\) to \\(\\frac{\\eta}{(1-\\mu)}\\) . If we make the approximation that the gradient is unchanging then \\[ -\\eta \\nabla L \\{1+\\mu+\\mu^2+...\\} = - \\frac{\\eta}{1-\\mu} \\nabla L \\] By contrast, in a region of high curvature in which gradient descent is oscillatory, successive contributions from the momentum term will tend to cancel and effective learning rate will be close to \\(\\eta\\) . We can now escape local minima or saddle points because we keep moving downwards even though the gradient of the mini-batch might be zero. Momentum can also help us reduce the oscillation of the gradients because the velocity vectors can smooth out these highly changing landscapes. It reduces the noise of the gradients and follows a more direct walk down the landscape. Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. 2013. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28 (ICML'13). JMLR.org, III–1139–III–1147.\nAdaptive Learning Rates # Different learning rate for each parameter.\nAdagrad # Adaptive gradient.\nAdaGrad reduces each learning rate parameter over time by using the accumulated sum of squares of all the derivates calculated for that parameter. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{r}^{t}}+\\delta} \\odot \\nabla L(\\mathbf{w}^{t-1}) \\] where \\(\\mathbf{r}^t\\) is the running sum of the squares of the gradients and \\(\\delta\\) is a small constant to ensure numerical stability. \\[ \\mathbf{r}^t = \\mathbf{r}^{t-1} + \\left(\\nabla L(\\mathbf{w}^{t})\\right)^2 \\] for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient r += dw*dw # Accumulated squared gradients w = w - lr * dw / (r.sqrt() + delta) PyTorch optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, eps=1e-10) We can see that when the gradient is changing very fast, the learning rate will be smaller. When the gradient is changing slowly, the learning rate will be bigger.\nA drawback of Adagrad is that as time goes by, the learning rate becomes smaller and smaller due to the monotonic increment of the running squared sum.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. J. Mach. Learn. Res. 12, null (2/1/2011), 2121–2159.\nRMSProp # Root Mean Square Propagation, Leaky AdaGrad\nSince AdaGrad accumulates the squared gradients from the beginning, the associatied weight updates can become very small as training progresses.\nRMSProp essentially replaces it with an exponentialy weighted average. \\[ \\mathbf{r}^t = \\alpha \\mathbf{r}^{t-1} + (1-\\alpha) \\left(\\nabla L(\\mathbf{w}^{t})\\right)^2 \\] where \\(0 \u003c \\alpha \u003c 1\\) . \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{r}^{t}}+\\delta} \\odot \\nabla L(\\mathbf{w}^{t-1}) \\] \\(\\) Typically we set the \\(\\alpha=0.9\\) . for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient r += alpha * r + (1-alpha) * dw*dw # Accumulated squared gradients w = w - lr * dw / (r.sqrt() + delta) PyTorch optimizer = torch.optim.RMSProp(model.parameters(), lr=0.01, alpha=0.99, eps=1e-8) Hinton, 2012. Neural Networks for Machine Learning. Lecture 6a.\nAdam # Adaptive moments.\nIf we combine RMSProp with momentum we ontain the most popular Adam optimization method.\nAdam maintains an exponentially weighted average of the first and the second moments. \\[ \\mathbf{s}^t = \\beta_1 \\mathbf{s}^{t-1} + (1-\\beta_1) \\left(\\nabla L(\\mathbf{w}^{t})\\right) \\] \\[ \\mathbf{r}^t = \\beta_2 \\mathbf{r}^{t-1} + (1-\\beta_2) \\left(\\nabla L(\\mathbf{w}^{t})\\right)^2 \\] We correct for the bias introduced by initializing \\(\\mathbf{s}^0\\) and \\(\\mathbf{r}^0\\) to zero. \\[ \\hat{\\mathbf{s}}^t = \\frac{\\mathbf{s}^t}{1-\\beta_1^t} \\] \\[ \\hat{\\mathbf{r}}^t = \\frac{\\mathbf{r}^t}{1-\\beta_2^t} \\] The updates are given as follows. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{\\mathbf{r}}^{t}}+\\delta} \\odot \\hat{\\mathbf{s}}^t \\] \\(\\) Typically we set the \\(\\beta_1=0.9\\) and \\(\\beta_2=0.99\\) . for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient s += beta1 * s + (1-beta1) * dw # Accumulated gradients r += beta2 * r + (1-beta2) * dw*dw # Accumulated squared gradients s_hat = s /(1-beta1**t) r_hat = r /(1-beta2**t) w = w - lr * s_hat / (r_hat.sqrt() + delta) PyTorch optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.99), eps=1e-08) Kingma, D.P. and Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\nAdamW # AdamW proposes a modification to Adam that improves regularization by adding weight decay. At each iteration we pull the parameters towards zero.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t} - \\eta \\lambda \\mathbf{w}^{t} \\] PyTorch optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9,0.99), eps=1e-08, weight_decay=0.01) Ilya Loshchilov, Frank Hutter, Decoupled Weight Decay Regularization, ICLR 2019.\nAdam and AdamW are the most widely used optimizers.\nLearning rate schedule # A small learning rate leads to slow convergence while a large learning rate leads to instability (due to divergent oscillations).\nIn practice we start with a large learning rate and and then reduce itover time.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla^{t-1} L(\\mathbf{w}^{t-1}) \\] Linear \\[ \\mathbf{\\eta}^t = \\left(1-\\frac{t}{K}\\right) \\mathbf{\\eta}^0 + \\left(\\frac{t}{K}\\right) \\mathbf{\\eta}^K \\] The learning rate reduces linearly over K steps, after which its value is held constant.\nPower \\[ \\mathbf{\\eta}^t = \\mathbf{\\eta}^0 \\left(1+\\frac{t}{s}\\right)^c \\] Exponential \\[ \\mathbf{\\eta}^t = \\mathbf{\\eta}^0 c^\\frac{t}{s}\\] from torch.optim import SGD from torch.optim.lr_scheduler import ExponentialLR optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9) scheduler = ExponentialLR(optimizer, gamma=0.9) PyTorch Learning Curve # Track the learning curve.\nParameter initialization # Initialization before starting the gradient descent.\nAvoid all parameters set to same value. (symmetry breaking)\nUniform distribution in the range \\([-\\epsilon,\\epsilon]\\) Zero-mean Gaussian \\(\\mathcal{N}(0,\\epsilon^2)\\) nn.init Collateral # https://pytorch.org/docs/stable/optim.html "},{"id":8,"href":"/docs/rl/","title":"Reinforcement Learning","section":"Docs","content":" Reinforcement Learning basics.\n"},{"id":9,"href":"/docs/training/backpropagation/","title":"Backpropagation","section":"Training deep neural networks","content":" Backpropagation # Backprop, Error Backpropagation.\nBackpropagation (or backprop) is an efficient technique to compute the gradient of the loss function.\nIt boils down to a local message passing scheme in which information is sent backwards through the network.\nForward propagation # stateDiagram-v2 direction LR z1: $$z_1$$ z2: $$z_2$$ zi: $$z_i$$ zM: $$...$$ aj: $$a_j=\\sum_i w_{ji} z_i$$ zj: $$z_j=h(a_j)$$ START1:::hidden --\u003e z1 START2:::hidden --\u003e z2 STARTi:::hidden --\u003e zi STARTM:::hidden --\u003e zM z1 --\u003e aj z2 --\u003e aj zi --\u003e aj:$$w_{ji}$$ zM --\u003e aj aj --\u003e zj zj --\u003e END:::hidden note left of aj : Pre-activation note left of zj : Activation classDef hidden display: none; Let\u0026rsquo;s consider a hidden unit in a general feed forward neural nework. \\[ a_j=\\sum_i w_{ji} z_i \\] where \\(z_i\\) is the activation of anoter unit or an input that sends an connection of unit \\(j\\) and \\(w_{ji}\\) is the weight associated with that connection. \\(a_j\\) is known as pre-activation and is transformed by a non-linear activation fucntion to give the activation \\(z_j\\) of unit \\(j\\) . \\[ z_j=h(a_j) \\] For any given data point in the traning set, we can pass the input and compute the activations of all the hidden and output units. This process is called forward propagation since it is the forward flow of information through the network.\nBackward propagation # \\[ \\frac{\\partial L_n}{\\partial w_{ji}} = \\frac{\\partial L_n}{\\partial a_{j}} \\frac{\\partial a_j}{\\partial w_{ji}} = \\delta_j z_i \\] where \\(\\delta_j\\) are referred to as errors \\[ \\frac{\\partial L_n}{\\partial a_{j}} := \\delta_j \\] and \\[ \\frac{\\partial a_j}{\\partial w_{ji}} = z_i \\] So we now have \\[ \\frac{\\partial L_n}{\\partial w_{ji}} = \\delta_j z_i \\] The required derivative is simply obtained by multiplying the value of \\(\\delta\\) for the unit at the output end of the weight by the value of \\(z\\) for the unit at the input end of the weight.\n\\(\\delta\\) for the output units are based on the losss function.\nTo evaluate the \\(\\delta\\) for the hidden units we again make use of the the chain rule for partial derivatives. \\[ \\delta_j := \\frac{\\partial L_n}{\\partial a_{j}} = \\sum_{k} \\frac{\\partial L_n}{\\partial a_{k}} \\frac{\\partial a_k}{\\partial a_{j}} \\] where the sum runs over all the units k to which j sends connections. \\[ \\delta_j = h^{'}(a_j)\\sum_{k} w_{kj} \\delta_k \\] This tells us that the value of \\(\\delta\\) for a particular hidden unit can be obtained by propagating the \\(\\delta\\) backward from uits higher up in the network.\nstateDiagram-v2 direction LR z1: $$z_1$$ z2: $$z_2$$ zi: $$z_i$$ zM: $$...$$ delta1: $$\\delta_1$$ delta2: $$\\delta_2$$ deltak: $$\\delta_k$$ deltaM: $$...$$ aj: $$a_j$$ zj: $$z_j$$ START1:::hidden --\u003e z1 START2:::hidden --\u003e z2 STARTi:::hidden --\u003e zi STARTM:::hidden --\u003e zM z1 --\u003e aj z2 --\u003e aj zi --\u003e aj:$$w_{ji}$$ zM --\u003e aj aj --\u003e zj zj --\u003e delta1 zj --\u003e delta2 zj --\u003e deltak:$$w_{kj}$$ zj --\u003e deltaM delta1 --\u003e zj delta2 --\u003e zj deltak --\u003e zj deltaM --\u003e zj delta1 --\u003e START11:::hidden delta2 --\u003e START22:::hidden deltak --\u003e STARTii:::hidden deltaM --\u003e STARTMM:::hidden note left of aj : Pre-activation note left of zj : Activation note right of deltak : Errors classDef hidden display: none; Forward propagation # For all hidden and ouput units compute in forward order\n\\[ a_j \\leftarrow \\sum_i w_{ji} z_i \\] \\[ z_j \\leftarrow h(a_j) \\] Error evaluation # For all output units compute\n\\[ \\delta_k \\leftarrow \\frac{\\partial L_n}{\\partial a_k} \\] Backward propagation # For all hidden units compute in reverse order\n\\[ \\delta_j \\leftarrow h^{'}(a_j)\\sum_{k} w_{kj} \\delta_k \\] \\[ \\frac{\\partial L_n}{\\partial w_{ji}} \\leftarrow \\delta_j z_i \\] Automatic differenciation # Algorithmic differentiation, autodiff, autograd\nThere are broadly 4 appoaches to compute derivatives.\nApproach Pros Cons Manual derivation of backprop equations. If done carefully can result in efficent code. Manual process, prone to erros and not easy to iterate on models Numerical evaluation of gradients via finite differences. Sometime sused to check for correctness of other methods. Limited by computational accuracy. Scales poorly with the size of the network. Symbolic differenciation using packages like sympy Closed form needed. Resulting expression can be very long (expression swell). Automatic differentiation Most prefered. Atılım Günes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. Automatic differentiation in machine learning: a survey. J. Mach. Learn. Res. 18, 1 (January 2017), 5595–5637.\nForward-mode automatic differentiation # We augment each intermediate variable \\(z_i\\) (known as primal variable) with an additional variable representing the value of some derivative of that variable, which we denote as \\(\\dot{z}_i\\) , known as tangent variable. The tangent variables are generated automatically.\nConsider the following function. \\[ f(x_1,x_2) = x_1x_2 + \\exp(x_1x_2) - \\sin(x_2) \\] When implemented in software the code consists of a sequence of operations than can be expressed as an evaluation trace of the underlying elementary operations. This trace can be visualized as a computation graph with respect to the following 7 primal variables. stateDiagram-v2 direction LR x1: $$x_1$$ x2: $$x_2$$ v1: $$v_1 = x_1$$ v2: $$v_2 = x_2$$ v3: $$v_3 = v_1v_2$$ v4: $$v_4 = \\sin(v_2)$$ v5: $$v_5 = \\exp(v_3)$$ v6: $$v_6 = v_3 - v_4$$ v7: $$v_7 = v_5 + v_6$$ f: $$f = v_5 + v_6$$ x1 --\u003e v1 x2 --\u003e v2 v1 --\u003e v3 v2 --\u003e v4 v2 --\u003e v3 v3 --\u003e v5 v4 --\u003e v6 v3 --\u003e v6 v5 --\u003e v7 v6 --\u003e v7 v7 --\u003e f We first write code to implement the evaluation of the primal variables. \\[ v_1 = x_1 \\] \\[ v_2 = x_2 \\] \\[ v_3 = v_1v_2 \\] \\[ v_4 = \\sin(v_2) \\] \\[ v_5 = \\exp(v_3) \\] \\[ v_6 = v_3 - v_4 \\] \\[ v7 = v_5 + v_6 \\] Not say we wish to evaluate the derivative \\(\\partial f/\\partial x_1\\) . First we define the tangent variables by \\[\\dot{v}_i = \\frac{\\partial v_i}{\\partial x_1}\\] Expressions for evaluating these can be constructed automatically using the chain rule of calculus. \\[ \\dot{v}_i = \\frac{\\partial v_i}{\\partial x_1} = \\sum_{j\\in\\text{parents}(i)} \\frac{\\partial v_i}{\\partial v_j} \\frac{\\partial v_j}{\\partial x_1} = \\sum_{j\\in\\text{parents}(i)} \\dot{v}_j \\frac{\\partial v_i}{\\partial v_j} \\] where \\(\\text{parents}(i)\\) denotes the set of parents of node i in the evaluation trace diagram.\nThe associated euqations and correspoding code for evaluating the tangent variables are generated automatically. \\[ \\dot{v}_1 = 1 \\] \\[ \\dot{v}_2 = 0 \\] \\[ \\dot{v}_3 = v_1\\dot{v}_2+\\dot{v}_1v_2 \\] \\[ \\dot{v}_4 = \\dot{v}_2\\cos(v_2) \\] \\[ \\dot{v}_5 = \\dot{v}_3\\exp(v_3) \\] \\[ \\dot{v}_6 = \\dot{v}_3 - \\dot{v}_4 \\] \\[ \\dot{v}_7 = \\dot{v}_5 + \\dot{v}_6 \\] To evaluate the derivative \\(\\frac{\\partial f}{\\partial x_1}\\) we input specific values of \\(x_1\\) and \\(x_2\\) and the code then executes the primal and tangent equations, numerically evalating the tuples \\((v_i,\\dot{v}_i)\\) in forward order untill we obtain the required derivative.\nThe forward mode with slight modifications can handle multiple outputs in the same pass but the proces has to be repeated for every parameter that we need the derivative. Since we are often in the rgime of one output with millions of parameters this is not scalable for modern deep neural networks. We therefore turn to an alternative version based on the backwards flow of derivative data through the evaluation trace graph.\nReverse-mode automatic differentiation # Reverse-mode automatic differentiation is a gernalization of the error backpropagation procedure we discussed earlier.\nAs with forward mode, we augment each primal variable \\(v_i\\) with an additional variable called adjoint variable, denoted as \\(\\bar{v}_i\\) . \\[\\bar{v}_i = \\frac{\\partial f}{\\partial v_i}\\] Expressions for evaluating these can be constructed automatically using the chain rule of calculus. \\[ \\bar{v}_i = \\frac{\\partial f}{\\partial v_i} = \\sum_{j\\in\\text{children}(i)} \\frac{\\partial f}{\\partial v_j} \\frac{\\partial v_j}{\\partial v_i} = \\sum_{j\\in\\text{children}(i)} \\bar{v}_j \\frac{\\partial v_j}{\\partial v_i} \\] where \\(\\text{children}(i)\\) denotes the set of children of node i in the evaluation trace diagram.\nThe successive evaluation of the adjoint variables represents a flow of information backwards through the graph. For multiple parameters a single backward pass is enough. Reverse mode is more memory intensive than forward mode.\n\\[ \\bar{v}_7 = 1 \\] \\[ \\bar{v}_6 = \\bar{v}_7 \\] \\[ \\bar{v}_5 = \\bar{v}_7 \\] \\[ \\bar{v}_4 = -\\bar{v}_6 \\] \\[ \\bar{v}_3 = \\bar{v}_5v_5+\\bar{v}_6 \\] \\[ \\bar{v}_2 = \\bar{v}_2v_1+\\bar{v}_4\\cos(v_2) \\] \\[ \\bar{v}_1 = \\bar{v}_3v_2 \\] Autograd in pytorch # A Gentle Introduction to torch.autograd The Fundamentals of Autograd "},{"id":10,"href":"/docs/training/normalization/","title":"Normalization","section":"Training deep neural networks","content":" Batch normalization # In batch normalization the mean and variance are computed across the mini-batch separately for each feature/hidden unit. For a mini-batch of size B \\[ \\mu_i = \\frac{1}{B} \\sum_{n=1}^{B} a_{ni} \\] \\[ \\sigma_i^2 = \\frac{1}{B} \\sum_{n=1}^{B} (a_{ni}-\\mu_i)^2 \\] We normalize the pre-activations as follows. \\[ \\hat{a}_{ni} = \\frac{a_{ni}-\\mu_i}{\\sqrt{\\sigma_i^2+\\delta}} \\] \\[ \\tilde{a}_{ni} = \\gamma_i \\hat{a}_{ni} + \\beta_i \\] PyTorch m = nn.BatchNorm1d(num_features) Layer normalization # In layer normalization the mean and variance are computed across the feature/hidden unit for each example seprately. \\[ \\mu_n = \\frac{1}{M} \\sum_{i=1}^{M} a_{ni} \\] \\[ \\sigma_n^2 = \\frac{1}{M} \\sum_{i=1}^{M} (a_{ni}-\\mu_i)^2 \\] We normalize the pre-activations as follows. \\[ \\hat{a}_{ni} = \\frac{a_{ni}-\\mu_n}{\\sqrt{\\sigma_n^2+\\delta}} \\] \\[ \\tilde{a}_{ni} = \\gamma_n \\hat{a}_{ni} + \\beta_n \\] PyTorch layer_norm = nn.LayerNorm(enormalized_shape) Collateral # https://pytorch.org/docs/stable/nn.html#normalization-layers\n"},{"id":11,"href":"/docs/training/regularization/","title":"Regularization","section":"Training deep neural networks","content":" Dropout # Early stopping # "},{"id":12,"href":"/docs/training/training_loop/","title":"Training loop","section":"Training deep neural networks","content":" Training loop # # Load the dataset. train_dataset = SampleDataset(X_train, y_train) test_dataset = SampleDataset(X_test, y_test) # Preparing your data for training with DataLoaders. batch_size = 64 train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True) # Define the model class. model = LogisticRegression(num_features=d) # Loss fucntion. loss_fn = nn.BCELoss() # Optimizer. optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9) # Learning rate scheduler. scheduler = ExponentialLR(optimizer, gamma=0.9) # Run for a few epochs. for epoch in range(n_epochs): # Iterate through the DataLoader to access mini-batches. for batch, (input, target) in enumerate(train_dataloader): # Prediction. output = model(input) # Compute loss. loss = loss_fn(output, target) # Compute gradient. loss.backward() # Gradient descent. optimizer.step() # Prevent gradient accumulation optimizer.zero_grad() # Adjust learning rate scheduler.step() "},{"id":13,"href":"/docs/training/activation_functions/","title":"Activation functions","section":"Training deep neural networks","content":" Sigmoid # ReLU # GeLU # GLU # Swish # SwiGLU # GLU Variants Improve Transformer\nWe offer no explanation as to why these architectures seem to work; we attribute their success, as all else, to divine benevolence\n"},{"id":14,"href":"/docs/training/quiz/","title":"Quiz","section":"Training deep neural networks","content":" Quiz # \\(\\) Derive the gradient of the loss function for linear regression and logistic regression.\nWhat is the most widely used optimizer ? What are the typically used parameters of the optimizer ?\nFor SGD with momemtum show that it increases the effective learning rate from \\(\\eta\\) to \\(\\frac{\\eta}{(1-\\mu)}\\) .\nIn Attention Is All You Need paper what is the optimizer and the learning rate scheduler used ?\nWhat is the disadvantage of forward-mode automatic differentiation ?\nWhat is the difference between batch and layer normalization ?\n"},{"id":15,"href":"/docs/training/coding/","title":"Coding","section":"Training deep neural networks","content":" Coding assignment # \\(\\) Setup # https://github.com/vikasraykar/deeplearning-dojo/\ngit clone https://github.com/vikasraykar/deeplearning-dojo.git cd deeplearning-dojo python -m venv .env source .env/bin/activate pip install -r requirements.txt Problem 1 # Linear Regression with numpy and batch gradient descent.\nIn the first coding assigment you will be implementing a basic Linear Regression model from scratch using only numpy. You will be implementing a basic batch gradient descent optimizer.\nYou can use only numpy and are not allowed to to use torch or any other python libraries.\nReview the linear regression model and its loss function. Given a feature matrix \\(\\mathbf{X}\\) as a \\(N \\times d\\) numpy.ndarray write the prediction and the loss function using matrix notation and carefully check for dimensions. Accont for the bias by appending the feature matrix with a column of ones. Derive the expression for the gradient of the loss function. Implement a basic batch gradient descent optimizer with a fixed learning rate first. Track the loss every few epochs and check the actual and estimated parameters. Check the MSE loss on the train and the test set. Implement a simple learning rate decay as follows lr=lr/(1+decay_factor*epoch). A sample stub is provided in the repo as below. Your task is to implement the predict and the fit function.\nLinearRegressionNumpy.py \u0026#34;\u0026#34;\u0026#34;Basic implementation of Linear Regression using only numpy. \u0026#34;\u0026#34;\u0026#34; import numpy as np class LinearRegression: \u0026#34;\u0026#34;\u0026#34;Linear Regression.\u0026#34;\u0026#34;\u0026#34; def __init__(self): pass def predict(self, X: np.ndarray) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34;Predction. Args: X (np.ndarray): Features matrix. (N,d) add_colum_vector_for_bias (bool, optional): Add a column vector of ones to model the bias term. Defaults to True. Returns: y (np.ndarray): Prediction vector. (N,) \u0026#34;\u0026#34;\u0026#34; pass def fit( self, X_train: np.ndarray, y_train: np.ndarray, learning_rate: float = 1e-3, learning_rate_decay: bool = True, learning_rate_decay_factor: float = 1.0, num_epochs: int = 100, track_loss_num_epochs: int = 100, ): \u0026#34;\u0026#34;\u0026#34;Training. Args: X_train (np.ndarray): Features matrix. (N,d) y_train (np.ndarray): Target vector. (N,) learning_rate (float, optional): Learning rate. Defaults to 0.001. learning_rate_decay (bool, optional): If True does learning rate deacy. Defaults to True. learning_rate_decay_factor (float, optional): The deacay factor (lr=lr/(1+decay_factor*epoch)). Defaults to 1.0. num_epochs (int, optional): Number of epochs. Defaults to 100. track_loss_num_epochs (int, optional): Compute loss on training set once in k epochs. Defaults to 100. \u0026#34;\u0026#34;\u0026#34; pass Problem 2 # Logistic Regression with numpy and batch gradient descent.\nIn the second coding assigment you will be implementing a basic Logisitc Regression model from scratch using only numpy. You will be implementing a basic batch gradient descent optimizer.\nYou can use only numpy and are not allowed to to use torch or any other python libraries.\nReview the logistic regression model and its loss function. Given a feature matrix \\(\\mathbf{X}\\) as a \\(N \\times d\\) numpy.ndarray write the prediction and the loss function using matrix notation and carefully check for dimensions. Accont for the bias by appending the feature matrix with a column of ones. Derive the expression for the gradient of the loss function. Modularize it so that it should look exactly like the derivative you got for linear regression. Implement a basic batch gradient descent optimizer with a fixed learning rate first. Track the loss every few epochs and check the actual and estimated parameters. Check the accuracy on the train and the test set. Implement a simple learning rate decay as follows lr=lr/(1+decay_factor*epoch). A sample stub is provided in the repo as below. Your task is to implement the predict_proba, predict and the fit function.\nLogisticRegressionNumpy.py \u0026#34;\u0026#34;\u0026#34;Basic implementation of Logistic Regression using numpy only.\u0026#34;\u0026#34;\u0026#34; import numpy as np class LogisticRegression: \u0026#34;\u0026#34;\u0026#34;Logistic Regression\u0026#34;\u0026#34;\u0026#34; def __init__(self): pass def fit( self, X_train: np.ndarray, y_train: np.ndarray, learning_rate: float = 1e-3, learning_rate_decay: bool = True, learning_rate_decay_factor: float = 1.0, num_epochs: int = 100, track_loss_num_epochs: int = 10, ): \u0026#34;\u0026#34;\u0026#34;Train. Args: X_train (np.ndarray): Feature matrix. (N,d) y_train (np.ndarray): Target labels (0,1). (N,) learning_rate (float, optional): The initial learning rate. Defaults to 1e-3. learning_rate_decay (bool, optional): If True enables learning rate decay. Defaults to True. learning_rate_decay_factor (float, optional): The learning rate decay factor (1/(1+decay_factor*epoch)). Defaults to 1.0. num_epochs (int, optional): The number of epochs to train. Defaults to 100. track_loss_num_epochs (int, optional): Compute loss on training set once in k epochs. Defaults to 10. \u0026#34;\u0026#34;\u0026#34; pass def predict_proba(self, X: np.ndarray): \u0026#34;\u0026#34;\u0026#34;Predict the probability of the positive class (Pr(y=1)). Args: X (np.ndarray): Feature matrix. (N,d) Returns: y_pred_proba (np.ndarray): Predicted probabilities. (N,) \u0026#34;\u0026#34;\u0026#34; pass def predict( self, X: np.ndarray, threshold: float = 0.5, ): \u0026#34;\u0026#34;\u0026#34;Predict the label(0,1). Args: X (np.ndarray): Feature matrix. (N,d) add_colum_vector_for_bias (bool, optional): Add a column vector of ones to model the bias term. Defaults to True. threshold (float, optional): The threshold on the probabilit. Defaults to 0.5. Returns: y (np.ndarray): Prediction vector. (N,) \u0026#34;\u0026#34;\u0026#34; pass Problem 3 # Logistic Regression with torch and min-batch SGD.\nReview Datasets \u0026amp; DataLoaders in pytorch. Experiment withe different optimizers covered in this lectures and plot the learning curve for different optimizers (SGD, SGD with momentum, AdaGrad, RMSProp, Adam), AdamW. Tune the learning rate for a optimizer. A sample stub is provided in the repo as below.\nLogisticRegressionPytorch.py Problem 4 # Logistic Regression with torch and min-batch SGD on a publicly avaiable dataset.\nChosee one publicly avaiable large dataset and implement custom datsets and loaders and learn either a linear regression model.\nReal Estate Data UAE\nBonus problem # AdamW implementation\nImplement the AdamW optimizer as a subclass of torch.optim.Optimizer.\nAn Optimizer subclass much implemnt two methods.\nimport torch class AdamW(torch.optim.Optimizer): def __init__(self,params, ...): pass def step(self): pass The PyTorch optimizer API has a few subtleties and study how some optimizers are written.\n"}]