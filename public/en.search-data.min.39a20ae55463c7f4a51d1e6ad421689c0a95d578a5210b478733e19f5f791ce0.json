[{"id":0,"href":"/docs/training/","title":"Training deep neural networks","section":"Docs","content":"The goal of training is to find the value of the parameters of a model to make effective predictions.\nWe choose the model parameters by optimizing a loss funcntion.\n"},{"id":1,"href":"/docs/training/model/","title":"Models","section":"Training deep neural networks","content":" Models # Linear Regression\nLogistic Regression\nMLP\nParameters # Loss functions # "},{"id":2,"href":"/docs/training/gradient_descent/","title":"Gradient Descent","section":"Training deep neural networks","content":" Gradient Descent # Steepest descent.\nLet \\(\\mathbf{w}\\) be a vector of all the parameters for a model.\nLet \\(L(\\mathbf{w})\\) be the loss function (or error function).\nWe need to choose the model parameters that optimizes (minimizes) the loss function.\n\\[ \\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}} L(\\mathbf{w}) \\] Let \\(\\nabla L(\\mathbf{w})\\) be the gradient vector, where each element is the partial derivative of the loss fucntion wrt each parameter.\nThe gradient vector points in the direction of the greatest rate of increase of the loss function.\nSo to mimimize the loss function we take small steps in the direction of \\(-\\nabla L(\\mathbf{w})\\) .\nAt the mimimum \\(\\nabla L(\\mathbf{w})=0\\) .\n\\(\\nabla L(\\mathbf{w})=0\\) .\nStationary points \\(\\nabla L(\\mathbf{w})=0\\) are knows as stationary points, which can be either a minima, maxima or a saddle point. The necessary and sufficient condition for a local minima is The gradient of the loss function should be zero. The Hessian matrix should be positive definite. For now we will assume the gradient is given. For deep neural networks the gradient can be computed efficiently via backpropagation (which we will revisit later).\nBatch Gradient Descent # We take a small step in the direction of the negative gradient.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L(\\mathbf{w}^{t-1}) \\] The parameter \\(\\eta \u003e 0\\) is called the learning rate and determines the step size at each iteration.\nThis update is repeated multiple times (till covergence).\nfor epoch in range(n_epochs): dw = gradient(loss, data, w) w = w - lr * dw Each step requires that the entire training data be processed to compute the gradient \\(\\nabla L(\\mathbf{w}^{t-1})\\) . For large datasets this is not comptationally efficient.\nStochastic Gradient Descent # In general most loss functions can be written as sum over each training instance. \\[ L(\\mathbf{w}) = \\sum_{i=1}^{N} L_i(\\mathbf{w}) \\] In Stochastic Gradient Descent (SGD) we update the parameters one data point at a time. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L_i(\\mathbf{w}^{t-1}) \\] A complete passthrough of the whole dataset is called an epoch. Training is done for multiple epochs depending on the size of the dataset.\nfor epoch in range(n_epochs): for i in range(n_data): dw = gradient(loss, data[i], w) w = w - lr * dw SGD is much faster and more computationally efficient, but it has noise in the estimation of the gradient. Since it updates the weight frequently, it can lead to big oscillations and that makes the training process highly unstable. Bottou, L. (2010). Large-Scale Machine Learning with Stochastic Gradient Descent. In: Lechevallier, Y., Saporta, G. (eds) Proceedings of COMPSTAT'2010. Physica-Verlag HD.\nMini-batch Stochastic Gradient Descent # Using a single example results in a very noisy estimate of the gradient. So we use a small random subset of data called mini-batch of size B (batch size) to compute the gradient.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L_{batch}(\\mathbf{w}^{t-1}) \\] for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) w = w - lr * dw PyTorch optimizer = optim.SGD(model.parameters(), lr=1e-3) Mini-batch SGD is the most commonly used method and is sometimes refered to as just SGD.\nTypical choices of the batch size are B=32,64,128,256,.. In practice we do a random shuffle of the data per epoch. In practice, mini-batch SGD is the most frequently used variation because it is both computationally cheap and results in more robust convergence.\nAdding momentum # One of the basic improvements over SGD comes from adding a momentum term.\nAt every time step, we update velocity by decaying the previous velocity by a factor of \\(0 \\leq \\mu \\leq 1\\) (called the momentum parameter) and adding the current gradient update. \\[ \\mathbf{v}^{t-1} \\leftarrow \\mu \\mathbf{v}^{t-2} - \\eta \\nabla L(\\mathbf{w}^{t-1}) \\] Then, we update our weights in the direction of the velocity vector. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} + \\mathbf{v}^{t-1} \\] for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient v = momentum * v - lr * dw # velocity w = w + v PyTorch optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) We now have two hyper-parameters learnign rate and momentum. Typically we set the momentum parameter to 0.9.\nEffective learning rate \\(\\) One interpretation of momentum to increase the effective learning rate from \\(\\eta\\) to \\(\\frac{\\eta}{(1-\\mu)}\\) . If we make the approximation that the gradient is unchanging then \\[ -\\eta \\nabla L \\{1+\\mu+\\mu^2+...\\} = - \\frac{\\eta}{1-\\mu} \\nabla L \\] By contrast, in a region of high curvature in which gradient descent is oscillatory, successive contributions from the momentum term will tend to cancel and effective learning rate will be close to \\(\\eta\\) . We can now escape local minima or saddle points because we keep moving downwards even though the gradient of the mini-batch might be zero. Momentum can also help us reduce the oscillation of the gradients because the velocity vectors can smooth out these highly changing landscapes. It reduces the noise of the gradients and follows a more direct walk down the landscape. Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. 2013. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28 (ICML'13). JMLR.org, III–1139–III–1147.\nAdaptive Learning Rates # Different learning rate for each parameter.\nAdagrad # Adaptive gradient.\nAdaGrad reduces each learning rate parameter over time by using the accumulated sum of squares of all the derivates calculated for that parameter. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{r}^{t}}+\\delta} \\odot \\nabla L(\\mathbf{w}^{t-1}) \\] where \\(\\mathbf{r}^t\\) is the running sum of the squares of the gradients and \\(\\delta\\) is a small constant to ensure numerical stability. \\[ \\mathbf{r}^t = \\mathbf{r}^{t-1} + \\left(\\nabla L(\\mathbf{w}^{t})\\right)^2 \\] for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient r += dw*dw # Accumulated squared gradients w = w - lr * dw / (r.sqrt() + delta) PyTorch optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, eps=1e-10) We can see that when the gradient is changing very fast, the learning rate will be smaller. When the gradient is changing slowly, the learning rate will be bigger.\nA drawback of Adagrad is that as time goes by, the learning rate becomes smaller and smaller due to the monotonic increment of the running squared sum.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. J. Mach. Learn. Res. 12, null (2/1/2011), 2121–2159.\nRMSProp # Root Mean Square Propagation, Leaky AdaGrad\nSince AdaGrad accumulates the squared gradients from the beginning, the associatied weight updates can become very small as training progresses.\nRMSProp essentially replaces it with an exponentialy weighted average. \\[ \\mathbf{r}^t = \\alpha \\mathbf{r}^{t-1} + (1-\\alpha) \\left(\\nabla L(\\mathbf{w}^{t})\\right)^2 \\] where \\(0 \u003c \\alpha \u003c 1\\) . \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{r}^{t}}+\\delta} \\odot \\nabla L(\\mathbf{w}^{t-1}) \\] \\(\\) Typically we set the \\(\\alpha=0.9\\) . for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient r += alpha * r + (1-alpha) * dw*dw # Accumulated squared gradients w = w - lr * dw / (r.sqrt() + delta) PyTorch optimizer = torch.optim.RMSProp(model.parameters(), lr=0.01, alpha=0.99, eps=1e-8) Hinton, 2012. Neural Networks for Machine Learning. Lecture 6a.\nAdam # Adaptive moments.\nIf we combine RMSProp with momentum we ontain the most popular Adam optimization method.\nAdam maintains an exponentially weighted average of the first and the second moments. \\[ \\mathbf{s}^t = \\beta_1 \\mathbf{s}^{t-1} + (1-\\beta_1) \\left(\\nabla L(\\mathbf{w}^{t})\\right) \\] \\[ \\mathbf{r}^t = \\beta_2 \\mathbf{r}^{t-1} + (1-\\beta_2) \\left(\\nabla L(\\mathbf{w}^{t})\\right)^2 \\] We correct for the bias introduced by initializing \\(\\mathbf{s}^0\\) and \\(\\mathbf{r}^0\\) to zero. \\[ \\hat{\\mathbf{s}}^t = \\frac{\\mathbf{s}^t}{1-\\beta_1^t} \\] \\[ \\hat{\\mathbf{r}}^t = \\frac{\\mathbf{r}^t}{1-\\beta_2^t} \\] The updates are given as follows. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{\\mathbf{r}}^{t}}+\\delta} \\odot \\hat{\\mathbf{s}}^t \\] \\(\\) Typically we set the \\(\\beta_1=0.9\\) and \\(\\beta_2=0.99\\) . for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient s += beta1 * s + (1-beta1) * dw # Accumulated gradients r += beta2 * r + (1-beta2) * dw*dw # Accumulated squared gradients s_hat = s /(1-beta1**t) r_hat = r /(1-beta2**t) w = w - lr * s_hat / (r_hat.sqrt() + delta) PyTorch optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.99), eps=1e-08) Kingma, D.P. and Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\nAdam is the most widely used optimizer.\nLearning rate schedule # A small learning rate leads to slow convergence while a large learning rate leads to instability (due to divergent oscillations).\nIn practice we start with a large learning rate and and then reduce itover time.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla^{t-1} L(\\mathbf{w}^{t-1}) \\] Linear \\[ \\mathbf{\\eta}^t = \\left(1-\\frac{t}{K}\\right) \\mathbf{\\eta}^0 + \\left(\\frac{t}{K}\\right) \\mathbf{\\eta}^K \\] The learning rate reduces linearly over K steps, after which its value is held constant.\nPower \\[ \\mathbf{\\eta}^t = \\mathbf{\\eta}^0 \\left(1+\\frac{t}{s}\\right)^c \\] Exponential \\[ \\mathbf{\\eta}^t = \\mathbf{\\eta}^0 c^\\frac{t}{s}\\] from torch.optim import SGD from torch.optim.lr_scheduler import ExponentialLR optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9) scheduler = ExponentialLR(optimizer, gamma=0.9) PyTorch Learning Curve # Track the learning curve.\nParameter initialization # Initialization before starting the gradient descent.\nAvoid all parameters set to same value. (symmetry breaking)\nUniform distribution in the range \\([-\\epsilon,\\epsilon]\\) Zero-mean Gaussian \\(\\mathcal{N}(0,\\epsilon^2)\\) nn.init Collateral # https://pytorch.org/docs/stable/optim.html "},{"id":3,"href":"/docs/training/backpropagation/","title":"Backpropagation","section":"Training deep neural networks","content":" Backpropagation # Backprop, Error Backpropagation.\nBackpropagation (or backprop) is an efficient technique to compute the gradient of the loss function.\nIt boils down to a local message passing scheme in which information is sent backwards through the network.\nForward propagation # stateDiagram-v2 direction LR z1: $$z_1$$ z2: $$z_2$$ zi: $$z_i$$ zM: $$...$$ aj: $$a_j=\\sum_i w_{ji} z_i$$ zj: $$z_j=h(a_j)$$ START1:::hidden --\u003e z1 START2:::hidden --\u003e z2 STARTi:::hidden --\u003e zi STARTM:::hidden --\u003e zM z1 --\u003e aj z2 --\u003e aj zi --\u003e aj:$$w_{ji}$$ zM --\u003e aj aj --\u003e zj zj --\u003e END:::hidden note left of aj : Pre-activation note left of zj : Activation classDef hidden display: none; Let\u0026rsquo;s consider a hidden unit in a general feed forward neural nework. \\[ a_j=\\sum_i w_{ji} z_i \\] where \\(z_i\\) is the activation of anoter unit or an input that sends an connection of unit \\(j\\) and \\(w_{ji}\\) is the weight associated with that connection. \\(a_j\\) is known as pre-activation and is transformed by a non-linear activation fucntion to give the activation \\(z_j\\) of unit \\(j\\) . \\[ z_j=h(a_j) \\] For any given data point in the traning set, we can pass the input and compute the activations of all the hidden and output units. This process is called forward propagation since it is the forward flow of information through the network.\nBackward propagation # \\[ \\frac{\\partial L_n}{\\partial w_{ji}} = \\frac{\\partial L_n}{\\partial a_{j}} \\frac{\\partial a_j}{\\partial w_{ji}} = \\delta_j z_i \\] where \\(\\delta_j\\) are referred to as errors \\[ \\frac{\\partial L_n}{\\partial a_{j}} := \\delta_j \\] and \\[ \\frac{\\partial a_j}{\\partial w_{ji}} = z_i \\] So we now have \\[ \\frac{\\partial L_n}{\\partial w_{ji}} = \\delta_j z_i \\] The required derivative is simply obtained by multiplying the value of \\(\\delta\\) for the unit at the output end of the weight by the value of \\(z\\) for the unit at the input end of the weight.\n\\(\\delta\\) for the output units are based on the losss function.\nTo evaluate the \\(\\delta\\) for the hidden units we again make use of the the chain rule for partial derivatives. \\[ \\delta_j := \\frac{\\partial L_n}{\\partial a_{j}} = \\sum_{k} \\frac{\\partial L_n}{\\partial a_{k}} \\frac{\\partial a_k}{\\partial a_{j}} \\] where the sum runs over all the units k to which j sends connections. \\[ \\delta_j = h^{'}(a_j)\\sum_{k} w_{kj} \\delta_k \\] This tells us that the value of \\(\\delta\\) for a particular hidden unit can be obtained by propagating the \\(\\delta\\) backward from uits higher up in the network.\nstateDiagram-v2 direction LR z1: $$z_1$$ z2: $$z_2$$ zi: $$z_i$$ zM: $$...$$ delta1: $$\\delta_1$$ delta2: $$\\delta_2$$ deltak: $$\\delta_k$$ deltaM: $$...$$ aj: $$a_j$$ zj: $$z_j$$ START1:::hidden --\u003e z1 START2:::hidden --\u003e z2 STARTi:::hidden --\u003e zi STARTM:::hidden --\u003e zM z1 --\u003e aj z2 --\u003e aj zi --\u003e aj:$$w_{ji}$$ zM --\u003e aj aj --\u003e zj zj --\u003e delta1 zj --\u003e delta2 zj --\u003e deltak:$$w_{kj}$$ zj --\u003e deltaM delta1 --\u003e zj delta2 --\u003e zj deltak --\u003e zj deltaM --\u003e zj delta1 --\u003e START11:::hidden delta2 --\u003e START22:::hidden deltak --\u003e STARTii:::hidden deltaM --\u003e STARTMM:::hidden note left of aj : Pre-activation note left of zj : Activation classDef hidden display: none; Forward propagation # For all hidden and ouput units compute in forward order\n\\[ a_j \\leftarrow \\sum_i w_{ji} z_i \\] \\[ z_j \\leftarrow h(a_j) \\] Error evaluation # For all output units compute\n\\[ \\delta_k \\leftarrow \\frac{\\partial L_n}{\\partial a_k} \\] Backward propagation # For all hidden units compute in reverse order\n\\[ \\delta_j \\leftarrow h^{'}(a_j)\\sum_{k} w_{kj} \\delta_k \\] \\[ \\frac{\\partial L_n}{\\partial w_{ji}} \\leftarrow \\delta_j z_i \\] Algorithmic differenciation # Forward mode # Reverse mode # "},{"id":4,"href":"/docs/training/normalization/","title":"Normalization","section":"Training deep neural networks","content":" Batch normalization # In batch normalization the mean and variance are computed across the mini-batch separately for each feature/hidden unit. For a mini-batch of size B \\[ \\mu_i = \\frac{1}{B} \\sum_{n=1}^{B} a_{ni} \\] \\[ \\sigma_i^2 = \\frac{1}{B} \\sum_{n=1}^{B} (a_{ni}-\\mu_i)^2 \\] We normalize the pre-activations as follows. \\[ \\hat{a}_{ni} = \\frac{a_{ni}-\\mu_i}{\\sqrt{\\sigma_i^2+\\delta}} \\] \\[ \\tilde{a}_{ni} = \\gamma_i \\hat{a}_{ni} + \\beta_i \\] PyTorch m = nn.BatchNorm1d(num_features) Layer normalization # In layer normalization the mean and variance are computed across the feature/hidden unit for each example seprately. \\[ \\mu_n = \\frac{1}{M} \\sum_{i=1}^{M} a_{ni} \\] \\[ \\sigma_n^2 = \\frac{1}{M} \\sum_{i=1}^{M} (a_{ni}-\\mu_i)^2 \\] We normalize the pre-activations as follows. \\[ \\hat{a}_{ni} = \\frac{a_{ni}-\\mu_n}{\\sqrt{\\sigma_n^2+\\delta}} \\] \\[ \\tilde{a}_{ni} = \\gamma_n \\hat{a}_{ni} + \\beta_n \\] PyTorch layer_norm = nn.LayerNorm(enormalized_shape) Collateral # https://pytorch.org/docs/stable/nn.html#normalization-layers\n"},{"id":5,"href":"/docs/training/regularization/","title":"Regularization","section":"Training deep neural networks","content":"Dropout\nEarly stopping\n"},{"id":6,"href":"/docs/training/training_loop/","title":"Training loop","section":"Training deep neural networks","content":" Training loop # from torch.optim import SGD from torch.optim.lr_scheduler import ExponentialLR optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9) scheduler = ExponentialLR(optimizer, gamma=0.9) for epoch in range(n_epochs): for input, target in dataset: optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step() scheduler.step() "},{"id":7,"href":"/docs/training/quiz/","title":"Quiz","section":"Training deep neural networks","content":" What is the most widely used optimizer ? What are the typically used parameters of the optimizer ? For SGD with momemtum show that it increases the effective learning rate from \\(\\eta\\) to \\(\\frac{\\eta}{(1-\\mu)}\\) . In Attention Is All You Need paper what is the optimizer and the learning rate scheduler used. "},{"id":8,"href":"/docs/training/coding/","title":"Coding","section":"Training deep neural networks","content":" Coding assignment # Plot learning curve for 3 differnt optimizers.\nnumpy\npytorch\n"}]