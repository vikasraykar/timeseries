[{"id":0,"href":"/docs/introduction/","title":"Explainability","section":"Docs","content":" Explainability # Explainability is the degree to which a human can understand the cause of a decision (or prediction) made by a prediction model 1.\nTime series tasks # Time series classification Time series forecasting Anomaly detection References # Interpretable Machine Learning: A Guide for Making Black Box Models Explainable, Christoph Molnar\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":1,"href":"/docs/forecasting/introduction/","title":"Introduction","section":"Forecasting","content":" Explainability for time series forecasting # Explainability is the degree to which a human can understand the cause of a decision (or prediction) made by a prediction model 1.\nTypes of time series # type time series only time series + external regressors univariate 1 ✔ 2 ✔ multiple univariate 3 ✔ 4 ✔ multiple hierarchical univariate 5 (phase 2) 6 (phase 2) multivariate 7 ✘ 8 ✘ Domain of explanations # what explain data Explain the time series based on trend, seasonality and cyclic patterns. forecaster Explains the trained forecaster based on the historical time series. forecast Explains the forecast at a certain point in time or a certain time interval. residual Explains the residual on the historical time series to understand where the forecaster is making errors. Scope of explanations # scope description global explanation Explains the forecaster trained on the historical time series. local explanation Explains the forecast made by a forecaster at a certain point in time. group explanation Explains the overall forecast made by a forecaster at a certain time interval. The individual local explanations can be used on each instance and then listed or aggregated for the entire interval. Type of explanations # type description features based Explanation is in terms of features that encode the time series (lag features, date encodings etc.) and external regressors. instance based Explanation is in terms of the importance or certain time points in the historical time series. Examples from supervised learning # ⚡️ features instance global feature importance plots SHAP values (what-if) Partial dependence plots local LIME prototypes and criticisms SHAP influence functions counterfactual explanation (what-if) counterfactual queries Model-agnostic explaninablity # We have access to only fit and predict methods of a forecaster. We have access to training data ? We have access to features ? Challenges # Non-iid nature of data. Robustness. If the the foreecast does not change drastically for the next time point the explanation should not change. Explain prediction intervals and quantile forecasts. Scalability. Interpretable models # Some time series forecasting models are inherently interprtable and we aim to wrap up the model specific interpretation whereever available.\nStatistical models Exponential Smoothing Holt-Winter S/ARIMA/X Croston\u0026rsquo;s models Theta Prohet machine learning models Tree based algorithms like XGBoost, CatBoost Gradient Boosted Machines like LightGBM Gaussian Process Regression deep learning models Recurrent Neural Networks (RNN) Temporal Convolutional Neural Networks (TCNN) Taxonomy # Inspired from here\nReferences # Interpretable Machine Learning: A Guide for Making Black Box Models Explainable, Christoph Molnar\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":2,"href":"/docs/metrics/","title":"Metrics","section":"Docs","content":" Metrics # Evaluating feature based local explanations # Let \\(f\\) be a black box predictor that maps an input \\(\\mathbf{x} \\in \\mathbb{R}^d\\) to an output \\(f(\\mathbf{x}) \\in \\mathbb{R}\\) .\nAn explanation function \\(g\\) takes in a predictor \\(f\\) and an instances \\(\\mathbf{x}\\) and returns the feature importance scores \\(g(f,\\mathbf{x}) \\in \\mathbb{R}^d\\) .\nLet \\(\\rho: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^{+}\\) be a distance metric over input instances.\nLet \\(D: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^{+}\\) be a distance metric over explanations.\nAn evaluation metric \\(\\mu\\) takes in as input a predictor \\(f\\) ,an explanation fucntion \\(g\\) , and input \\(\\mathbf{x}\\) , and outputs a scalar \\(\\mu(f,g;\\mathbf{x})\\) .\nWe wil mainly focus on these threes evaluation metrics which can be evaluated without access to ground-truth explanations1.\nFaithfulness # (high) faithfulness,relevance,fidelity. The feature importance scores from \\(g\\) should correspond to the importance features of \\(\\mathbf{x}\\) for \\(f\\) , such that, when we set a particular set of features \\(\\mathbf{x}_s\\) to a baseline value \\(\\overline{\\mathbf{x}}_s\\) , the change in predictor\u0026rsquo;s output should be proportional (measured via correlation) to the sum of the attribution scores of features in \\(\\mathbf{x}_s\\) .\nFor a subset of indices \\(S \\subset {1,2,...,d}\\) , let \\(\\mathbf{x}_s = ( \\mathbf{x}_i,i \\in S )\\) a sub-vector of input features. For a given subset size \\(|S|\\) , we define faithfullness as\n\\( \\mu_{F}(f,g,|S|;\\mathbf{x}) = \\text{corr}_{S \\in \\binom {d}{|S|}}\\left( \\sum_{i \\in S}g(f,\\mathbf{x})_{i},f(\\mathbf{x})-f(\\mathbf{x}|\\mathbf{x}_s=\\overline{\\mathbf{x}}_s)\\right) \\) The baseline can be the mean of the training data.\nSensitivity # (low) sensitivity, stability, reliability, explanation continuity. If inputs are near each other and their model outputs are similar, then their explanations should be close to each other.\nLet \\(\\mathcal{N}_r(\\mathbf{x})\\) be a neighborhood of datapoints within a radius \\(r\\) of \\(\\mathbf{x}\\) .\n\\( \\mathcal{N}_r(\\mathbf{x}) = \\left\\{ \\mathbf{z} \\in \\mathcal{D}_x | \\rho(\\mathbf{x},\\mathbf{z}) \\leq r, f(\\mathbf{x}) = f(\\mathbf{z}) \\right\\} \\) Max Sensitivity\n\\( \\mu_{M}(f,g,r;\\mathbf{x}) = \\max_{z\\in\\mathcal{N}_r(\\mathbf{x})} D(g(f,\\mathbf{x}),g(f,\\mathbf{z})) \\) Average Sensitivity\n\\( \\mu_{A}(f,g,r;\\mathbf{x}) = \\int_{\\mathcal{N}_r(\\mathbf{x})} D(g(f,\\mathbf{x}),g(f,\\mathbf{z})) \\mathbb{P}_{\\mathbf{x}}(\\mathbf{z}) d\\mathbf{z} \\) Complexity # (low) complexity,information gain,sparsity. A complex explantion is one that uses all the \\(d\\) features in its explanation. The simplest explanation would be concentrated on one feature.\nWe define complexity as the entropy of the fractional contribution distribution.\n\\( \\mu_{C}(f,g;\\mathbf{x}) = \\mathbb{E}_{i}\\left[ -\\ln(\\mathbb{P}_{g})\\right] = - \\sum_{i=1}^{d} \\mathbb{P}_{g}(i) \\ln(\\mathbb{P}_{g}(i)) \\) where \\(\\mathbb{P}_{g}\\) is the fractional contribution distribution\n\\( \\mathbb{P}_{g}(i) = \\frac{|g(f,\\mathbf{x})_i|}{\\sum_{j=1}^{d}|g(f,\\mathbf{x})_j|}. \\) References # Evaluating and Aggregating Feature-based Model Explanations, Bhatt, Umang and Weller, Adrian and Moura, José M. F., Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI-20), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":3,"href":"/docs/forecasting/timeshap/","title":"TimeSHAP","section":"Forecasting","content":" TimeSHAP # code API docs example notebook TimeSHAP is a feature based post-hoc black box explainer to explain the forecast of any univariate time series forecaster using tree-based regressors to build the surrogate model and SHAP(SHapley Additive exPlanations) values for the explanations. For any given univariate time series we first generate a sequence of backtested historical forecasts using an (expanding window) splitter. Using the splitter we split the time series into a sequence of train an test splits. The expanding window splitter uses more and more training data, while keeping the test window size fixed to the forecast horizon. The method is model agnostic in the sense that it needs access to only the fit and predict methods of the forecaster. The forecaster is trained on the train split and evaluated on the test split and all the test split predictions are concatenated to get the backtested historical forecasts for each step of the forecast horizon. We the construct a surrogate time series forecasting task by reducing it to a standard supervised regression problem. For each time point we generate a set of interpretable features (lag features, seasonal features, date time encodings etc) based on which we need to predict the backtested forecasted time series values. The surrogate model is fitted using tree-based regressors like XGBoost, CatBoost,LightGBM etc. Explanation is in now in terms of features that encode the time series. We mainly rely on the TreeSHAP (SHapley Additive exPlanations) algorithm to explain the output of ensemble tree models. In order to improve the sensitivity we extend the above approach by aggregating multiple explanations from bootstrapped versions of the time series.\nTime series forecasting # A univariate time series is a series with a single time-dependent variable. Let \\(f(t):\\mathbb{Z}\\to\\mathbb{R}^1\\) represent a latent univariate time series for any discrete time index \\(t \\in \\mathbb{Z}\\) . We observe a sequence of historical noisy (and potentially missing) values \\(y(t)\\) for \\(t \\in [1,\\dots,T]\\) such that in expectation \\(\\mathbb{E}[y(t)]=f(t)\\) . For example, in the retail domain \\(y(t)\\) could represent the daily sales of a product and \\(f(t)\\) the true latent demand for the product.\nThe task of time series forecasting is to estimate \\(f(t)\\) for all \\(t \u003eT\\) based on the observed historical time series \\(y(t)\\) for \\(t \\in [1,\\dots,T]\\) . When we talk about the forecast, we usually mean the average value of the forecast distribution also known as the point forecast, \\(f(t)\\) , which is the mean ( \\(\\mathbb{E}[y(t)]\\) ) of the forecast distribution. The time series forecast is typically done for a fixed number or periods in the future, refered to as the forecast horizon, \\(h\\) . Let \\(\\hat{f}(T+h)\\) for \\(h \\in [1,\\dots,H]\\) be the forecasted time series for the forecast horizon \\(h\\) based on the historical observed time series \\(y(1),...,y(T)\\) . \\( \\textbf{forecaster model}\\:\\:\\hat{f}(T+h|y(1),...,y(T))\\:\\text{for}\\:h=1,...,H \\) Using the language of supervised learning, \\(y(1),...,y(T)\\) is the training data of \\(T\\) samples based on which we learn/train a forecaster model \\(\\hat{f}\\) . The trained model \\(\\hat{f}\\) is then used to predict/forecast on the test set of \\(H\\) time points in the future. Unlike supervised learing where the model is usually fit once in time series forecasting for many algorithms we will have to continously fit the model before forecasting as more recent data arrives.\nExplainability for forecasting # Explainability is the degree to which a human can understand the cause of a decision (or prediction) made by a prediction model 123.Various notions of explainability (local and global explanations) and explainer algorithms has been studied in classical supervised learning paradigms like classification and regression.\nScope of explanations # An explanation is the answer to either a why-question or a what if-scenario. We define the following three notions of explanations in the context of time series forecasting.\nA local explantion explains the forecast made by a forecaster at a certain point in time. Why is the forecasted sales on July 22, 2019 much higher than the average sales? Will the forecast increase if I increase the offered discount? A global explanation explains the forecaster trained on the historical time series. What are the most important attributes the forecaster relies on to make the forecast? What is the impact of diccount on the sales forecast? A semi-local explanation explains the overall forecast made by a forecaster in a certain time interval. In general this returns one (semi-local) explanation aggregated over all the multiple time steps in the forecast horizon. Why is the forecasted sales over the next 4 weeks much higher? Type of explanations # In the context of time series the explanations can boradly of the following two types.\nFeatures based Explanation is in terms of features that encode the time series (lag features, date encodings etc.) and external regressors. Instance based Explanation is in terms of the importance or certain time points in the historical time series. Type of explainers # An explainer is an algorithm the generates local, semi-local and global explanations for a forecasting algorithm. We can boradly categorize explainers into the following 3 types.\nDirectly interpretable explainers Some time series forecasting algorithms are inherently interpretable by desgin. For example, for an autoregressive model of order \\(p\\) ( \\(AR(p)\\) ) the coefficients correponds to the importance associated with the past \\(p\\) values of the time series. Another example is, Prophet which uses a decomposable additive time series model with three main model components: trend, seasonality, and holidays. The explanation is directly in terms of these 3 components. White-box explainers Though not directly interpretable, some forecasting algorithms can be explained if we have access to the internals of the corresponding forecasting algorithm. For example, for deep neural forecasting algorithms if we have access to the model we can compute saliency maps and activations to explain the inner working of the model. Tree based regression ensembles like XGBoost, CatBoost and LightGBM gprovide global explanations in terms of the feature importance scores. Black-box explainers Such explainers are model agnostic and generally require access to model\u0026rsquo;s predict (and sometimes fit) functions. Given a source black box model, such explainers generally train a surrogate model that is explainable. TimeSHAP # TimeSHAP is a feature based post-hoc black box explainer to explain the forecast of any univariate time series forecaster using tree-based regressors to build the surrogate model and SHAP(SHapley Additive exPlanations) values for the explanations. Surrogate model # The method is model agnostic in the sense that it needs access to only the fit and predict methods of the forecaster. At any time \\(t\\) using all the historical data available so far (that its, \\(y(1),...,y(t)\\) ) we can train the forecaster model \\(f\\) using the fit method. Once the forecaster is trained we can then generate the corresponding \\(H\\) (forecast horizon) forecasts \\( f(t+h|t)=f(t+h|y(1),...,y(t))\\:\\text{for}\\:h=1,...,H, \\) time steps ahead using the predict method. Our goal now is to learn a surrogate model(s) \\(g\\) to predict the forecasts from the forecaster. \\( g(t+h|t)=g(t+h|y(1),...,y(t))\\:\\text{for}\\:h=1,...,H, \\) While the original forecaster learns to predict \\(y(t+h)\\) based on \\(y(1),...,y(t)\\) the surrogate model is trained to predict the forecasts \\(f(t+h)\\) made by the forecaster. Essentially we want to mimic the forecaster using a surrogate model. We choose the surrogate model that can be easily interpreted.\nBacktested historical forecasts # In order to generate data to train the surrogate model we use backtesting. For any given univariate time series we first generate a sequence of backtested historical forecasts using an expanding window splitter. Using the splitter we split the time series into a sequence of train and test splits. The expanding window splitter uses more and more training data, while keeping the test window size fixed to the forecast horizon. The forecaster is trained on the train split and evaluated on the test split and all the test split predictions are concatenated to get the backtested historical forecasts for each step of the forecast horizon.\nThis is one of the most computationally expensive steps since for a time series of length \\(n\\) we will have to potentially invoke `fit` and `predict` roughly \\(O(n)\\) times to generate the backtested forecasts. - This can be effficiently parallelized since each task can be executed independently. In our implementation we use [ray](https://github.com/ray-project/ray) to parallelize this. - For computationally expensive `fit` models it may be cheaper to forecast without full refitting. However we may need to pass the context/historical time series so far to do the forecast. Some forecasters have a light weight `update` method has the same signature as train but does not do refitting and only does minimal context updates to make the prediction. - Certain forecasting algorithms train one large model based on large number of multiple univariate time series. In such scenarios we completely avoid refitting the model and rely only on the `predict` method to generate the in-sample forecasts and build a surrogate model. While this avoids backtesting and can potentially overfit, since the model is trained or large number of time series this may be fine. Regressor reduction # We now have an original time series and the corresponding backtested forecast time series for each step of the forecast horizon. The goal of the surrogate model is to learn to predict the backtested forecast time series based on on the original time series. We construct a surrogate time series forecasting task by reducing it to a standard supervised regression problem.\nA common machine learning approach to time series forecasting is to reduce it to a standard supervised regression problem. A standard supervised regression task takes as input a \\(d\\) -dimensional feature vector \\(\\mathbf{x}\\in\\mathbb{R}^d\\) and predicts a scalar \\(y \\in \\mathbb{R}\\) . The regressor \\(y = f(\\mathbf{x})\\) is learnt based on a labelled training dataset \\(\\left(\\mathbf{x}_i,y_i\\right)\\) , for \\(i=1,..,n\\) samples. However there is no direct concept of input features ( \\(\\mathbf{x}\\) ) and output target ( \\(y\\) ) for a time series. Instead, we must choose the backtested time series forecast values as the variable to be predicted and use various time series feature engineering techniques (like lag features, date time encodings etc.) to construct the features from the original time series.\nInterpretable features # Essentially, for each time point \\(t\\) we generate a feature vector \\(\\mathbf{x}(t) \\in\\mathbb{R}^d\\) based on the original time series values observed so far based on which we need to predict the backtested forecast time series for each step in the forecast horizon \\(y(t) \\in\\mathbb{R}\\) . The table below is a list of features we use. See here for more details. We generally like the features to be interpretable in the sense that the end user of the explanation should be able to comprehend the meaning of these features.\nThe feature vector \\(\\mathbf{x}(t)\\) needs to be constructed only based on the time step \\(t\\) and the historical values of the time series \\(y(1),...,y(t-1)\\) and should not use the current time series value \\(y(t)\\) . feature name description sales(t-3) The value of the time series (sales) at the (t-3) previous time step. sales(t-2) The value of the time series (sales) at the (t-2) previous time step. sales(t-1) The value of the time series (sales) at the (t-1) previous time step. sales(t-2*365) The value of the time series (sales) at the (t-2*365) previous time step. sales(t-1*365) The value of the time series (sales) at the (t-1*365) previous time step. sales_min(t-1,t-3) The min of the past 3 values in the sales time series. sales_mean(t-1,t-3) The mean of the past 3 values in the sales time series. sales_max(t-1,t-3) The max of the past 3 values in the sales time series. sales_min(0,t-1) The min of all the values so far in the sales time series. sales_mean(0,t-1) The mean of all the values so far in the sales time series. sales_max(0,t-1) The max of all the values so far in the sales time series. year The year. month The month name of the year from January to December. day_of_year The ordinal day of the year from 1 to 365. day_of_month The ordinal day of the month from 1 to 31. week_of_year The ordinal week of the year from 1 to 52. week_of_month The ordinal week of the month from 1 to 4. day_of_week The day of the week from Monday to Sunday. is_weekend Indicates whether the date is a weekend or not. quarter The ordinal quarter of the date from 1 to 4. season The season Spring/Summer/Fall/Winter. fashion_season The fashion season Spring/Summer (January to June) or Fall/Winter (July to December). is_month_start Indicates whether the date is the first day of the month. is_month_end Indicates whether the date is the last day of the month. is_quarter_start Indicates whether the date is the first day of the quarter. is_quarter_end Indicates whether the date is the last day of the quarter. is_year_start Indicates whether the date is the first day of the year. is_year_end Indicates whether the date is the last day of the year. is_leap_year Indicates whether the date belongs to a leap year. hour The hours of the day. minute The minutes of the hour. second The seconds of the minute. holiday-IN Indicates whether the date is a IN holiday or not. t Feature to model simple polynomial (of degree 1) trend in sales. t^2 Feature to model simple polynomial (of degree 2) trend in sales. **External regressors** Classical time series forecasting algorithms esentially learn a model to forecast based on the historical values of the time series. In many domains, the value of the time series depends on several external time series which we refer to as related external regressors. For example in the retail domain, the sales is potentially influence by discount, promotion, events, weather etc. Each external regressor in itself is a time series and some methods allow to explicity include external regressors to improve forecasting. If external regressors are avaiable we can also encode them as interpretable features using similar features as above. Typically some exeternal regressors can be **forwad looking**. For example, the discount that will be used iin the future is typically planned by the retailer in advance. feature name description discount(t-3) The value of the time series (discount) at the (t-3) previous time step. discount(t-2) The value of the time series (discount) at the (t-2) previous time step. discount(t-1) The value of the time series (discount) at the (t-1) previous time step. discount(t) The value of the time series (discount) at the current time step. Multi-step forecasting # In the last section we described some of the commonly used methods to transform the original time series to a set of interpretable features. Recall that we have \\(H\\) backtested time series forecats which are to be used as targets to learn the surrogate regressor model. Here we will desxribe common strategies4 that can be used used for multi-step forecasting to regression reduction.\nRecursive # A single regressor model is fit for one-step-ahead forecast horizon and then called recursively to predict multiple steps ahead. Let \\(\\mathcal{G}(y(1),...,y(t))\\) be the one-step ahead surrogate forecaster that has been learnt based on the training data, where the forecaster predicts the one-step ahead forecast using features based on the time series values till \\(t\\) , that is, \\(y(1),...,y(t)\\) . The forecasts from the surrogate models are made recusively as follows, \\( g(t+1|t) = \\mathcal{G}(y(1),...,y(t)). \\) For \\(h=2,3,...,H\\) \\( g(t+h|t) = \\mathcal{G}(y(1),...,y(t),g(t+1|t),...,g(t+h-1|t)). \\) For example,\nhorizon forecast strategy 1 \\( g(t+1|t) \\) \\( \\mathcal{G}(y(1),...,y(t)) \\) 2 \\( g(t+2|t) \\) \\( \\mathcal{G}(y(1),...,y(t),g(t+1|t)) \\) h \\( g(t+h|t) \\) \\( \\mathcal{G}(y(1),...,y(t),g(t+1|t),...,g(t+h-1|t)) \\) A well-known drawback of the recursive method is its sensitivity to the estimation error, since estimated values, instead of actual ones, are more and more used when we get further in the future forecasts. Direct # A separate regressor model is fit for each step ahead in the forecast horizon and then independently applied to predict multiple steps ahead. Let \\(\\mathcal{G}_h(y(1),...,y(t))\\) be the h-step ahead surrogate forecaster that has been learnt based on the training data, where the forecaster predicts the h-step ahead forecast using features based on the time series values till \\(t\\) , that is, \\(y(1),...,y(t)\\) . The forecasts are made directly as follows, \\( g(t+h|t) = \\mathcal{G}_h(y(1),...,y(t))\\text{ for }h=1,2,...,H \\) For example,\nhorizon forecast strategy 1 \\( g(t+1|t) \\) \\( \\mathcal{G}_1(y(1),...,y(t)) \\) 2 \\( g(t+2|t) \\) \\( \\mathcal{G}_2(y(1),...,y(t)) \\) h \\( g(t+h|t) \\) \\( \\mathcal{G}_h(y(1),...,y(t)) \\) Since the Direct strategy does not use any approximated values to compute the forecasts, it is not prone to any accumulation of errors. However since the models are learned independently no statistical dependencies between the predictions is considered. This strategy also demands a large computational time since the number of models to learn is equal to the size of the forecast horizon. DirRec # The DirRec strategy combines the architectures and the principles underlying the Direct and the Recursive strategies. DirRec computes the forecasts with different models for every horizon (like the Direct strategy) and, at each time step, it enlarges the set of inputs by adding variables corresponding to the forecasts of the previous step (like the Recursive strategy).\n\\( g(t+1|t) = \\mathcal{G}_1(y(1),...,y(t)) \\) For \\(h=2,3,...,H\\) \\( g(t+h|t) = \\mathcal{G}_h(y(1),...,y(t),g(t+1|t),...,g(t+h-1|t)) \\) For example,\nhorizon forecast strategy 1 \\( g(t+1|t) \\) \\( \\mathcal{G}_1(y(1),...,y(t)) \\) 2 \\( g(t+2|t) \\) \\( \\mathcal{G}_2(y(1),...,y(t),g(t+1|t)) \\) h \\( g(t+h|t) \\) \\( \\mathcal{G}_h(y(1),...,y(t),g(t+1|t),...,g(t+h-1|t)) \\) Tree ensemble regressors # So far, for each time point we generate a set of interpretable features (lag features, seasonal features, date time encodings etc) based on which we need to predict the backtested forecasted time series values. The surrogate model is then fitted using tree-based regressors like XGBoost, CatBoost,LightGBM etc.\nSHapley Additive exPlanations # While in general we can use any regressor we prefer tree-based ensembles like XGBoost, CatBoost and LightGBM since they are resonably accurate and more importantly support fast explaninablity algorithms like TreeSHAP which we reply on for the various explanations. Explanation is in now in terms of features that encode the time series. We mainly rely on the TreeSHAP (SHapley Additive exPlanations) algorithm to explain the output of ensemble tree models. SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. 56\nGlobal explanations # A global explanation explains the forecaster trained on the historical time series. The explanation type can be one of following four types.\nNote that the recurisve strategy has only one model, while the direct and the dirrec strategies have \\(H\\) models corresponding to each step of the forecasting horizon. Hence there will be a possible seprate global explanation for each model based on the forecast horizon. SHAP feature importance # The importance score for each features based on the shap values. Specifically this is the mean absolute value of the shap values for each feature across the entire dataset. To get an overview of which features are most important for a model we can compute the SHAP values of every feature for every sample. We can then take the mean absolute value of the SHAP values for each feature to get a feature importance score for each feature.\nFeature importance # The relative contribution of each feature to the model. A higher value of this score when compared to another feature implies it is more important for generating a forecast. Feature importance provides a score that indicates how useful or valuable each feature was in the construction of the boosted decision trees within the model. The more an attribute is used to make key decisions with decision trees, the higher its relative importance. Feature importance is calculated for a single decision tree by the amount that each attribute split point improves the performance measure, weighted by the number of observations the node is responsible for. The feature importances are then averaged across all of the the decision trees within the model. The gain is the most relevant attribute to interpret the relative importance of each feature. The gain implies the relative contribution of the corresponding feature to the model calculated by taking each feature’s contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction.\nPartial dependence plot # The partial dependence plot (PDP) for a feature shows the marginal effect the feature has on the forecast (from the surrogate model). The PDP shows how the average prediction in your dataset changes when a particular feature is changed. The partial dependence function at a particular feature value represents the average prediction if we force all data points to assume that feature value.\nThe calculation for the partial dependence plots has a causal interpretation. One way to think about PDP is that it is an **intervention query**. We intervene on a feature and measure the changes in the predictions. In doing so, we analyze the causal relationship between the feature and the prediction. The assumption of independence is the biggest issue with PDP plots. It is assumed that the feature(s) for which the partial dependence is computed are not correlated with other features. SHAP dependence plot # The shap dependence plot (SDP) for each feature shows the mean shap value for a particular features across the entire dataset. This shows how the model depends on the given feature, and is like a richer extenstion of the classical partial dependence plots.\nLocal explanations # A local explantion explains the forecast made by a forecaster at a certain point in time.\nSHAP explanation # SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations. While SHAP values can explain the output of any machine learning model, high-speed exact algorithms are available for tree ensemble methods\nThe SHAP explanation shows features contributing to push the forecasted sales from the base value (the average sales) to the forecaster model output. Features pushing the forecast higher are shown in blue and those pushing the forecast lower are in red.\nThis time instance (Mon Jul 1 00:00:00 2019) has a forecasted sales (5028.76), 2579.59 units higher than the average (2449.17) mainly because of the discount(t) (40.50) and sales(t-1) (4242.54) while sales(t-2*52) (4375.00) was trying to push it lower.\nLocal partial dependence plot # The (local) PDP for a given feature shows how the forecast (from the surrogate model) varies as the feature value changes.\nLocal SHAP dependence plot # The (local) SDP for a given feature shows how the shap value varies as the feature value changes.\nSemi-local explanations # A semi-local explanation explains the overall forecast made by a forecaster in a certain time interval. In general this returns one (semi-local) explanation aggregated over all the multiple time steps in the forecast horizon.\nSHAP feature importance # The importance score for each feature which is the corresponding shap value for that feature.\nPartial dependence plot # The PDP for a given feature shows how the forecast (from the surrogate model) varies as the feature value changes.\nSHAP dependence plot # The SDP for a given feature shows how the shap value varies as the feature value changes.\nExplaining prediction intervals # The same setup can also be used to explain the width of the prediction interval. Instead of regressing on the mean forecast from the forecaster we can regress on the width of the prediction interval.\nBootstrapped ensemble # In order to improve the sensitivity we extend the above approach by aggregating multiple explanations from bootstrapped versions of the time series.\nExamples # Naive # The forecast is the value of the last observation. \\( f(t+h|t) = y(t),\\text{ for }h=1,2,... \\) aix360ts from aix360ts.forecasting.forecasters import Naive from aix360ts.forecasting.explainers import TimeSHAP forecaster = Naive() explainer = TimeSHAP(forecaster=forecaster) forecaster global - SHAP global - PDP local - SHAP local - PDP SeasonalNaive # The forecast is the value of the last observation from the same season of the year. For example, with monthly data, the forecast for all future February values is equal to the last observed February value.\naix360ts from aix360ts.forecasting.forecasters import SeasonalNaive from aix360ts.forecasting.explainers import TimeSHAP forecaster = SeasonalNaive(m=52) explainer = TimeSHAP(forecaster=forecaster) forecaster global - SHAP global - PDP local - SHAP local - PDP MovingAverage # A moving average forecast of order \\(k\\) , or, \\(MA(k)\\) , is the mean of the last \\(k\\) observations of the time series.\naix360ts from aix360ts.forecasting.forecasters import MovingAverage from aix360ts.forecasting.explainers import TimeSHAP forecaster = MovingAverage(k=6) explainer = TimeSHAP(forecaster=forecaster) forecaster global - SHAP global - PDP local - SHAP local - PDP Simple Exponential Smoothing # The forecast is the exponentially weighted average of its past values. The forecast can also be interpreted as a weighted average between the most recent observation and the previous forecast.\naix360ts from aix360ts.forecasting.forecasters import SES from aix360ts.forecasting.explainers import TimeSHAP forecaster = SES(alpha=0.5) explainer = TimeSHAP(forecaster=forecaster) forecaster global - SHAP global - PDP local - SHAP local - PDP Prophet # Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.\naix360ts from aix360ts.forecasting.forecasters import Prophet from aix360ts.forecasting.explainers import TimeSHAP forecaster = Prophet() explainer = TimeSHAP(forecaster=forecaster) forecaster global - SHAP global - PDP local - SHAP local - PDP XGBoost # Forecasting to Regression reduction using XGBoost.\naix360ts from aix360ts.forecasting.forecasters import RegressorReduction from aix360ts.forecasting.explainers import TimeSHAP forecaster = RegressorReduction() explainer = TimeSHAP(forecaster=forecaster) forecaster global - SHAP global - PDP local - SHAP local - PDP References # Interpretable Machine Learning: A Guide for Making Black Box Models Explainable, Christoph Molnar\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nExplanation in Artificial Intelligence: Insights from the Social Sciences, Tim Miller\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTowards A Rigorous Science of Interpretable Machine Learning, Finale Doshi-Velez, Been Kim\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBontempi G., Ben Taieb S., Le Borgne YA. (2013) Machine Learning Strategies for Time Series Forecasting. In: Aufaure MA., Zimányi E. (eds) Business Intelligence. eBISS 2012. Lecture Notes in Business Information Processing, vol 138. Springer, Berlin, Heidelberg.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/slundberg/shap\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLundberg, S.M., Erion, G., Chen, H. et al. From local explanations to global understanding with explainable AI for trees. Nat Mach Intell 2, 56–67 (2020).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":4,"href":"/docs/forecasting/features/","title":"Interpretable features","section":"Forecasting","content":" Encoding time series as interpretable features # A common machine learning approach to time series forecasting is to reduce it to a standard supervised regression problem. A regression task takes as input a \\(d\\) -dimensional feature vector \\(\\mathbf{x}\\in\\mathbb{R}^d\\) and predicts a scalar \\(y \\in \\mathbb{R}\\) . The regressor \\(y = f(\\mathbf{x})\\) is learnt based on a labelled training dataset \\(\\left(\\mathbf{x}_i,y_i\\right)\\) , for \\(i=1,..,n\\) samples. However there is do direct concept of input features ( \\(\\mathbf{x}\\) ) and output target ( \\(y\\) ) for a time series. Instead, we must choose the time series values to be forecasted as the variable to be predicted and use various feature engineering to construct the features that will be used to make predictions for future time steps. For each time point \\(t\\) we generate a feature vector \\(\\mathbf{x}(t) \\in\\mathbb{R}^d\\) based on which we need to predict the observed time series value \\(y(t) \\in\\mathbb{R}\\) . Here we describe some of the commonly used methods to transform a time series to feature matrix.\nThe feature vector \\(\\mathbf{x}(t)\\) needs to be constructed only based on the time step \\(t\\) and the historical values of the time series \\(y(1),...,y(t-1)\\) and should not use the current time series value \\(y(t)\\) . Lag features # code example The value of the time series at previous time steps. Lag features are the classical way that time series forecasting problems are transformed into supervised learning problems.\naix360ts from aix360ts.transformers import LagFeatures transformer = LagFeatures(lags=3) input sales date 2017-01-01 21.0 2017-01-02 18.0 2017-01-03 9.0 2017-01-04 18.0 2017-01-05 15.0 ... ... 2019-12-27 923.0 2019-12-28 1194.0 2019-12-29 1341.0 2019-12-30 920.0 2019-12-31 748.0 [1095 rows x 1 columns] ouput sales(t-3) sales(t-2) sales(t-1) date 2017-01-01 NaN NaN NaN 2017-01-02 NaN NaN 21.0 2017-01-03 NaN 21.0 18.0 2017-01-04 21.0 18.0 9.0 2017-01-05 18.0 9.0 18.0 ... ... ... ... 2019-12-27 796.0 1178.0 852.0 2019-12-28 1178.0 852.0 923.0 2019-12-29 852.0 923.0 1194.0 2019-12-30 923.0 1194.0 1341.0 2019-12-31 1194.0 1341.0 920.0 [1095 rows x 3 columns] features description type sales(t-3) The value of the time series (sales) at the (t-3) previous time step. continuous sales(t-2) The value of the time series (sales) at the (t-2) previous time step. continuous sales(t-1) The value of the time series (sales) at the (t-1) previous time step. continuous Seasonal lag features # code example The value of the time series at time steps for the previous seasons. For example, with monthly data, the feature for February is equal to the last observed February value.\naix360ts from aix360ts.transformers import SeasonalLagFeatures transformer = SeasonalLagFeatures(lags=2, m=365) input sales date 2017-01-01 21.0 2017-01-02 18.0 2017-01-03 9.0 2017-01-04 18.0 2017-01-05 15.0 ... ... 2019-12-27 923.0 2019-12-28 1194.0 2019-12-29 1341.0 2019-12-30 920.0 2019-12-31 748.0 [1095 rows x 1 columns] ouput sales(t-2*365) sales(t-1*365) date 2017-01-01 NaN NaN 2017-01-02 NaN NaN 2017-01-03 NaN NaN 2017-01-04 NaN NaN 2017-01-05 NaN NaN ... ... ... 2019-12-27 428.0 463.0 2019-12-28 440.0 607.0 2019-12-29 700.0 778.0 2019-12-30 894.0 1038.0 2019-12-31 828.0 531.0 [1095 rows x 2 columns] features description type sales(t-2*365) The value of the time series (sales) at the (t-2*365) previous time step. continuous sales(t-1*365) The value of the time series (sales) at the (t-1*365) previous time step. continuous Rolling window features # code example Rolling window statistics (mean,max,min).\naix360ts from aix360ts.transformers import RollingWindowFeatures transformer = RollingWindowFeatures(window=3) input sales date 2017-01-01 21.0 2017-01-02 18.0 2017-01-03 9.0 2017-01-04 18.0 2017-01-05 15.0 ... ... 2019-12-27 923.0 2019-12-28 1194.0 2019-12-29 1341.0 2019-12-30 920.0 2019-12-31 748.0 [1095 rows x 1 columns] ouput sales_min(t-1,t-3) sales_mean(t-1,t-3) sales_max(t-1,t-3) date 2017-01-01 NaN NaN NaN 2017-01-02 NaN NaN NaN 2017-01-03 NaN NaN NaN 2017-01-04 9.0 16.000000 21.0 2017-01-05 9.0 15.000000 18.0 ... ... ... ... 2019-12-27 796.0 942.000000 1178.0 2019-12-28 852.0 984.333333 1178.0 2019-12-29 852.0 989.666667 1194.0 2019-12-30 923.0 1152.666667 1341.0 2019-12-31 920.0 1151.666667 1341.0 [1095 rows x 3 columns] features description type sales_min(t-1,t-3) The min of the past 3 values in the sales time series. continuous sales_mean(t-1,t-3) The mean of the past 3 values in the sales time series. continuous sales_max(t-1,t-3) The max of the past 3 values in the sales time series. continuous Expanding window features # code example Expanding window statistics (mean,max,min).\naix360ts from aix360ts.transformers import ExpandingWindowFeatures transformer = ExpandingWindowFeatures() input sales date 2017-01-01 21.0 2017-01-02 18.0 2017-01-03 9.0 2017-01-04 18.0 2017-01-05 15.0 ... ... 2019-12-27 923.0 2019-12-28 1194.0 2019-12-29 1341.0 2019-12-30 920.0 2019-12-31 748.0 [1095 rows x 1 columns] ouput sales_min(0,t-1) sales_mean(0,t-1) sales_max(0,t-1) date 2017-01-01 21.0 21.000000 21.0 2017-01-02 18.0 19.500000 21.0 2017-01-03 9.0 16.000000 21.0 2017-01-04 9.0 16.500000 21.0 2017-01-05 9.0 16.200000 21.0 ... ... ... ... 2019-12-27 9.0 364.901008 1907.0 2019-12-28 9.0 365.660256 1907.0 2019-12-29 9.0 366.552608 1907.0 2019-12-30 9.0 367.058501 1907.0 2019-12-31 9.0 367.406393 1907.0 [1095 rows x 3 columns] features description type sales_min(0,t-1) The min of all the values so far in the sales time series. continuous sales_mean(0,t-1) The mean of all the values so far in the sales time series. continuous sales_max(0,t-1) The max of all the values so far in the sales time series. continuous Date features # code example Date related features.\naix360ts from aix360ts.transformers import DateFeatures transformer = DateFeatures(encode_cyclical_features=False) input sales date 2017-01-01 21.0 2017-01-02 18.0 2017-01-03 9.0 2017-01-04 18.0 2017-01-05 15.0 ... ... 2019-12-27 923.0 2019-12-28 1194.0 2019-12-29 1341.0 2019-12-30 920.0 2019-12-31 748.0 [1095 rows x 1 columns] ouput year month day_of_year day_of_month week_of_year week_of_month ... is_month_end is_quarter_start is_quarter_end is_year_start is_year_end is_leap_year date ... 2017-01-01 2017 January 1 1 52 1 ... no yes no yes no no 2017-01-02 2017 January 2 2 1 1 ... no no no no no no 2017-01-03 2017 January 3 3 1 1 ... no no no no no no 2017-01-04 2017 January 4 4 1 1 ... no no no no no no 2017-01-05 2017 January 5 5 1 1 ... no no no no no no ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2019-12-27 2019 December 361 27 52 4 ... no no no no no no 2019-12-28 2019 December 362 28 52 4 ... no no no no no no 2019-12-29 2019 December 363 29 52 5 ... no no no no no no 2019-12-30 2019 December 364 30 1 5 ... no no no no no no 2019-12-31 2019 December 365 31 1 5 ... yes no yes no yes no [1095 rows x 18 columns] features description type year The year. ordinal month The month name of the year from January to December. cyclical day_of_year The ordinal day of the year from 1 to 365. cyclical day_of_month The ordinal day of the month from 1 to 31. cyclical week_of_year The ordinal week of the year from 1 to 52. cyclical week_of_month The ordinal week of the month from 1 to 4. cyclical day_of_week The day of the week from Monday to Sunday. cyclical is_weekend Indicates whether the date is a weekend or not. binary quarter The ordinal quarter of the date from 1 to 4. cyclical season The season Spring/Summer/Fall/Winter. categorical fashion_season The fashion season Spring/Summer (January to June) or Fall/Winter (July to December). categorical is_month_start Indicates whether the date is the first day of the month. binary is_month_end Indicates whether the date is the last day of the month. binary is_quarter_start Indicates whether the date is the first day of the quarter. binary is_quarter_end Indicates whether the date is the last day of the quarter. binary is_year_start Indicates whether the date is the first day of the year. binary is_year_end Indicates whether the date is the last day of the year. binary is_leap_year Indicates whether the date belongs to a leap year. binary Time features # code example Time related features.\naix360ts from aix360ts.transformers import TimeFeatures transformer = TimeFeatures(encode_cyclical_features=False) input sales date 2017-01-01 21.0 2017-01-02 18.0 2017-01-03 9.0 2017-01-04 18.0 2017-01-05 15.0 ... ... 2019-12-27 923.0 2019-12-28 1194.0 2019-12-29 1341.0 2019-12-30 920.0 2019-12-31 748.0 [1095 rows x 1 columns] ouput hour minute second date 2017-01-01 0 0 0 2017-01-02 0 0 0 2017-01-03 0 0 0 2017-01-04 0 0 0 2017-01-05 0 0 0 ... ... ... ... 2019-12-27 0 0 0 2019-12-28 0 0 0 2019-12-29 0 0 0 2019-12-30 0 0 0 2019-12-31 0 0 0 [1095 rows x 3 columns] features description type hour The hours of the day. cyclical minute The minutes of the hour. cyclical second The seconds of the minute. cyclical Encoding Cyclical Features # May time attributes like month, day_of_year, hour etc. all occur in specific cycles and are refered to as cyclical features. One way to encode cyclical features is via an ordinal scale. For example, month is typically encoded via an ordinal scale from 1(January) to 12(December).\nThe main problem with ordinal scale is that the distance between two feature values does not reflect the true cyclical nature of the data. For example, November and January are equidistant to December, while in the ordinal scale the absolute distance between November and December is 1 while that between December and January if 11. While this may work reasonably well for certain algorithms sometime it is benefical to encode the cyclical feature to reflect the cyclical nature of the attribute.\nOne method commonly used for encoding a cyclical feature is to perform a sine and cosine transformation of the feature. For each feature \\(x\\) which takes ordinal values from \\(1,...,K\\) we use a pair of transformed features. \\[ x_{sin} = \\sin\\left(\\frac{2\\pi (x-1)}{K}\\right)\\quad x_{cos} = \\cos\\left(\\frac{2\\pi (x-1)}{K}\\right)\\quad\\text{for}\\quad x=1,...,K \\] Note that is essentially maps the values around a circle. As an added benefit, it is also scaled to the range [-1, 1] which will also aid convergence for neural networks.\nHoliday features # code example Encode country specific holidays as features. We use the python holidays package.\nA buffer can also be specified before and after the holiday using a tapering triangular window. aix360ts from aix360ts.transformers import HolidayFeatures transformer = HolidayFeatures(country=\u0026#34;IN\u0026#34;, buffer=2, include_holiday_name=True) input sales date 2017-01-01 21.0 2017-01-02 18.0 2017-01-03 9.0 2017-01-04 18.0 2017-01-05 15.0 ... ... 2019-12-27 923.0 2019-12-28 1194.0 2019-12-29 1341.0 2019-12-30 920.0 2019-12-31 748.0 [1095 rows x 1 columns] ouput holiday-IN holiday-IN-name date 2017-01-01 0.000000 no 2017-01-02 0.000000 no 2017-01-03 0.000000 no 2017-01-04 0.000000 no 2017-01-05 0.000000 no 2017-01-06 0.000000 no 2017-01-07 0.000000 no 2017-01-08 0.000000 no 2017-01-09 0.000000 no 2017-01-10 0.000000 no 2017-01-11 0.000000 no 2017-01-12 0.333333 no 2017-01-13 0.666667 no 2017-01-14 1.000000 Makar Sankranti / Pongal 2017-01-15 0.666667 no 2017-01-16 0.333333 no 2017-01-17 0.000000 no 2017-01-18 0.000000 no 2017-01-19 0.000000 no 2017-01-20 0.000000 no 2017-01-21 0.000000 no 2017-01-22 0.000000 no 2017-01-23 0.000000 no 2017-01-24 0.333333 no 2017-01-25 0.666667 no 2017-01-26 1.000000 Republic Day 2017-01-27 0.666667 no 2017-01-28 0.333333 no 2017-01-29 0.000000 no 2017-01-30 0.000000 no 2017-01-31 0.000000 no 2017-02-01 0.000000 no 2017-02-02 0.000000 no 2017-02-03 0.000000 no 2017-02-04 0.000000 no 2017-02-05 0.000000 no 2017-02-06 0.000000 no 2017-02-07 0.000000 no 2017-02-08 0.000000 no 2017-02-09 0.000000 no 2017-02-10 0.000000 no 2017-02-11 0.000000 no 2017-02-12 0.000000 no 2017-02-13 0.000000 no 2017-02-14 0.000000 no 2017-02-15 0.000000 no 2017-02-16 0.000000 no 2017-02-17 0.000000 no 2017-02-18 0.000000 no 2017-02-19 0.000000 no features description type holiday-IN Indicates whether the date is a IN holiday or not. continuous holiday-IN-name The holiday name. categorical Trend features # code example Features to model simple polynomial trend. Adds features of the form \\(t,t^2,..\\) . High degrees can cause overfitting, do not go above two unless needed.\naix360ts from aix360ts.transformers import TrendFeatures transformer = TrendFeatures(degree=3) input sales date 2017-01-01 21.0 2017-01-02 18.0 2017-01-03 9.0 2017-01-04 18.0 2017-01-05 15.0 ... ... 2019-12-27 923.0 2019-12-28 1194.0 2019-12-29 1341.0 2019-12-30 920.0 2019-12-31 748.0 [1095 rows x 1 columns] ouput sales_trend_linear sales_trend_quadratic sales_trend_cubic date 2017-01-01 0 0 0 2017-01-02 1 1 1 2017-01-03 2 4 8 2017-01-04 3 9 27 2017-01-05 4 16 64 ... ... ... ... 2019-12-27 1090 1188100 1295029000 2019-12-28 1091 1190281 1298596571 2019-12-29 1092 1192464 1302170688 2019-12-30 1093 1194649 1305751357 2019-12-31 1094 1196836 1309338584 [1095 rows x 3 columns] features description type sales_trend_linear Feature to model simple polynomial (of degree 1) trend in sales. continuous sales_trend_quadratic Feature to model simple polynomial (of degree 2) trend in sales. continuous sales_trend_cubic Feature to model simple polynomial (of degree 3) trend in sales. continuous References # https://www.avanwyk.com/encoding-cyclical-features-for-deep-learning/ "},{"id":5,"href":"/docs/forecasting/accuracy-metrics/","title":"Accuracy metrics","section":"Forecasting","content":" Forecast accuracy metrics # A compilation of various metrics used to measure forecast accuracy\ngist code repo Let $y(t)$ be the actual observation for time period $t$ and let $f(t)$ be the forecast for the same time period. Let $n$ the length of the training dataset (number of historical observations), and $h$ the forecasting horizon.\nForecast errors # Based on how we measure the forecast error $e(t)$ at a time point $t$ several metrics are defined. A forecast error is the difference between an observed value and its forecast.\nacronym error type equation scale independent *E error \\(e(t)=y(t)-f(t)\\) no *PE percentage error \\(pe(t)=\\frac{y(t)-f(t)}{y(t)}\\) yes *RE relative error \\(re(t)=\\frac{y(t)-f(t)}{y(t)-f^*(t)}\\) yes *SE scaled error \\(se(t)=\\frac{y(t)-f(t)}{s}\\) yes We are evaluating the accuracy over $h$ forecasting time steps. The final forecast accuracy metric is then an aggregation (via mean, median etc.) over over $h$ time steps.\nacronym aggregation M* Mean MA* Mean Absolute Md* Median MdA* Median Absolute GM* Geometric Mean GMA* Geometric Mean Absolute MS* Mean Squared RMS* Root Mean Squared {M|Md|GM}{-|A|S}{E|PE|RE|SE}\nTLDR # While there are several metrics we recommend the following 6 metrics to be definitely included in a forecasting toolkit.\nacronym name comments MAE Mean Absolute Error For assessing accuracy on a a single time series use MAE because it is easiest to interpret. However, it cannot be compared across series because it is scale dependent. RMSE Root Mean Square Error A forecast method that minimises the MAE will lead to forecasts of the median, while minimising the RMSE will lead to forecasts of the mean. Hence RMSE is also widely used, despite being more difficult to interpret. MAPE Mean Absolute Percentage Error MAPE has the advantage of being scale independent and hence can be used to compare forecast performance between different time series. However MAPE has the disadvantage of being infinite or undefined if there are zero values in the time series, as is frequent for intermittent demand data. MASE Mean Absolute Scaled Error MASE is a scale free error metric and can be used to compare forecast methods on a single time series and also to compare forecast accuracy between series. It is also well suited for intermittent demand time series since is never gives infinite or undefined values. wMAPE (volume) weighted Mean Absolute Percentage Error The is MAPE which is weighted by the volume. MSPL Mean Scaled Pinball Loss The metric to use for quantile forecasts. Categorizing metrics # category description Accuracy How close are the forecasts to the true value? Bias Is there a systematic under- or over-forecasting? Error Variance The spread of the point forecast error around the mean. Accuracy Risk The variability in the accuracy metric over multiple forecasts. Stability The degree to twhich forecasts remain unchanged subject to minor variations in the underlying data. Scale dependent metrics # Let $e(t)$ be the one-step forecast error, which is the difference between the observation $y(t)$ and the forecast made using all observations up to but not including $y(t)$.\n\\( e(t)=y(t)-f(t) \\) By scale dependent we mean that the value of the metric depends on the scale of the data.\nME # Mean Error\nME \\( \\text{ME}=\\text{mean}\\left(e(t)\\right) \\) notes - ME is likely to be small since positive and negative errors tend to offset one another. - ME will tell you if there is a systematic under- or over-forecasting, called the **forecast bias**. - ME does not give much indication as the the size of the typical errors. MAE # Mean Absolute Error\nMAE \\( \\text{MAE}=\\text{mean}\\left(|e(t)|\\right) \\) notes - MAE has the advantage of being more interpretable and is easier to explain to users. - MAE is also sometimes referred to as MAD (Mean Absolute Deviation). - A forecast method that minimises the MAE will lead to forecasts of the median, while minimising the RMSE will lead to forecasts of the mean. MdAE # Median Absolute Error\nMdAE \\( \\text{MdAE}=\\text{median}\\left(|e(t)|\\right) \\) notes - MdAE has the advantage of being more interpretable and is easier to explain to users. GMAE # Geometric Mean Absolute Error\nGMAE \\( \\text{GMAE}=\\text{gmean}\\left(|e(t)|\\right) \\) notes - GMAE has the flaw of being equal to zero when any of the error terms are zero. - GMAE is same as GRMSE because the square root and the square cancel each other in a geometric mean. MSE # Mean Square Error\nMSE \\( \\text{MSE}=\\text{mean}\\left(e(t)^2\\right) \\) notes - MSE has the advantage of being easier to handle mathematically and is commonly used in loss functions for optimization. - MSE tends to penalize large errors over smaller ones. RMSE # Root Mean Square Error\nRMSE \\( \\text{RMSE}=\\sqrt{\\text{mean}\\left(e(t)^2\\right)} \\) notes - RMSE has the advantage of being easier to handle mathematically and is commonly used in loss functions for optimization. - RMSE tends to penalize large errors over smaller ones. - A forecast method that minimises the MAE will lead to forecasts of the median, while minimising the RMSE will lead to forecasts of the mean. Percentage error metrics # For scale dependent metrics that the value of the metric depends on the scale of the data. Therefore, they do not facilitate comparison across different time series and for different time intervals.\nFirst we define a relative or percentage error as\n\\( pe(t)=\\frac{y(t)-f(t)}{y(t)} \\) Percentage errors have the advantage of being scale independent and hence useful to compare performance between different time series.\nWhen the time series values are very close to zero (especially for intermittent demand data), the relative or percentage error is meaningless and is actually not defined when the value is zero. MPE # Mean Percentage Error\nMPE \\( \\text{MPE}=\\text{mean}\\left(pe(t)\\right) \\) notes - MPE is likely to be small since positive and negative errors tend to offset one another. - MPE will tell you if there is a systematic under- or over-forecasting, called the **forecast bias**. - MPE does not give much indication as the the size of the typical errors. MAPE # Mean Absolute Percentage Error\nMAPE \\( \\text{MAPE}=\\text{mean}\\left(|pe(t)|\\right) \\) notes - MAPE puts heavier penalty on positive errors than on negative errors. - This observation has led to the use of symmetric MAPE or [sMAPE](#smape). MdAPE # Median Absolute Percentage Error\nMdAPE \\( \\text{MdAPE}=\\text{median}\\left(|pe(t)|\\right) \\) notes sMAPE # Symmetric Mean Absolute Percentage Error\nsMAPE \\( \\text{sMAPE}=\\text{mean}\\left(2\\cdot\\left|\\frac{y(t)-f(t)}{y(t)+f(t)}\\right|\\right) \\) notes - MAPE puts heavier penalty on positive errors than on negative errors. - This observation has led to the use of symmetric MAPE or sMAPE. - sMAPE which was used in the M3 forecasting competition. sMdAPE # Symmetric Median Absolute Percentage Error\nsMdAPE \\( \\text{sMdAPE}=\\text{median}\\left(2\\cdot\\left|\\frac{y(t)-f(t)}{y(t)+f(t)}\\right|\\right) \\) notes MSPE # Mean Square Percentage Error\nMSPE \\( \\text{MSE}=\\text{mean}\\left(pe(t)^2\\right) \\) notes RMSPE # Root Mean Square Percentage Error\nRMSPE \\( \\text{MSE}=\\sqrt{\\text{mean}\\left(pe(t)^2\\right)} \\) notes MAAPE # Mean Absolute Arctangent Percent Error\nMAAPE \\( \\text{MAAPE}= \\text{mean} \\left(\\text{arctan}\\left(|pe(t)|\\right)\\right) = \\text{mean} \\left(\\text{tan}^{-1}\\left(|pe(t)|\\right)\\right) \\) notes - MAPE produces infinite or undefined values when the actual values are zero or close to zero, which is a common with intermittent data in retail. While other alternate measures have been proposed to deal with this disadvantage of MAPE, it still remains the preferred method of business forecasters following its intuitive interpretation as absolute percentage error. Mean Absolute Arctangent Percent Error is an alternative measure that has the same interpretation as an absolute percentage error (APE) but can overcome MAPE’s disadvantage of generating infinite values. - **Boundedness** MAAPE is bounded and it varies from 0 to $\\pi/2$. MAAPE does not go to infinity even with close-to-zero actual values, which is a significant advantage of MAAPE over MAPE. - - **Robustness** Absolute Arctangent Percent Error given by $AAPE = \\text{arctan}\\left(|pe(t)|\\right)$ converges to $\\pi/2$ for large forecast errors, thus limits the influence of outliers, which often distort the calculation of the overall forecast accuracy. Therefore, MAAPE can be particularly useful if there are extremely large forecast errors as a result of mistaken or incorrect measurements. - However, if the extremely large forecast errors are considered as genuine variations that might have some important business implications, rather than being due to mistaken or incorrect measurements, MAAPE would not be appropriate. - MAAPE is also asymmetric but has a more balanced penalty between positive and negative errors than MAPE. references Kim, Sungil, and Heeyoung Kim. [A new metric of absolute percentage error for intermittent demand forecasts](https://core.ac.uk/reader/82178886). International Journal of Forecasting 32.3 (2016): 669-679. Relative error metrics # An alternative to percentage error for the calculation of scale-independent metrics involves dividing each error by the error obtained using a baseline forecasting algorithm (typically the naive baseline $f^*(t)=y(t-1)$).\n\\( re(t)=\\frac{e(t)}{e^*(t)}=\\frac{y(t)-f(t)}{y(t)-f^*(t)} \\) MARE # Mean Absolute Relative Error\nMARE \\( \\text{MARE}=\\text{mean}\\left(|re(t)|\\right) \\) notes MdARE # Median Absolute Relative Error\nMdARE \\( \\text{MdARE}=\\text{median}\\left(|re(t)|\\right) \\) notes GMARE # Geometric Mean Absolute Relative Error\nGMARE \\( \\text{GMARE}=\\text{gmean}\\left(|re(t)|\\right) \\) notes U-statistic # Theil\u0026rsquo;s $U$-statistic\nU-statistic \\( \\text{U}= \\sqrt{\\frac{A}{B}},\\text{where}, \\) \\( \\quad\\text{A}=\\sum_{t=1}^{n-1}\\left(\\frac{y(t+1)-f(t+1)}{y(t)}\\right)^2\\quad\\text{B}=\\sum_{t=1}^{n-1}\\left(\\frac{y(t+1)-y(t)}{y(t)}\\right)^2 \\) notes - Large errors given more weight then small errors and also provides a relative basis for comparison with naive (**NF1** forecast $f(t)=y(t-1)$) methods. - The numerator is similar to the MAPE and the denominator to the MAPE of the naive forecast method. - Smaller the better and should be less than 1. - $U=1$ The naive method is as good as the forecasting technique being evaluated. - $U\u003c1$ The forecasting technique being evaluated is better than the naive method. The smaller the $U$-statistic, the better the forecasting method is relative to the naive method. - $U\u003e1$ There is no point in using the forecasting method, since using a naive method will produce better results. Scaled error metrics # This generally involves scaling the error term by a suitable scale parameter.\n\\( se(t)=\\frac{e(t)}{s}=\\frac{y(t)-f(t)}{s} \\) MAD/Mean # MAD/Mean ratio\nMAD/Mean \\( \\text{MAD/Mean}=\\text{mean}\\left(|se(t)|\\right),\\text{where } s = \\frac{1}{n}\\sum_{i=1}^{n} y(t) \\) notes - The scale parameter $s$ is the *in-sample* mean of the time series. \\( s = \\frac{1}{n}\\sum_{i=1}^{n} y(t) \\) - Assumes stationarity, that is, the mean is stable over time, which is generally not true for data which shows trend, seasonality, or other patterns. references wMAPE # (volume) weighted Mean Absolute Percentage Error\nwMAPE \\( \\text{wMAPE}=\\text{mean}\\left(|se(t)|\\right),\\text{where } s = \\sum_{i=n+1}^{n+h} |y(t)| \\) notes - The scale parameter $s$ is the sum of the absolute value of the time series we are evaluation on (sometimes also called *volume*). \\( s = \\sum_{i=n+1}^{n+h} |y(t)| \\) - So essentially wMAPE is the Sum of Absolute errors divided by the Sum of the Actuals. - This is equivalent to weighing the percentage error $pe(t)$ by a scale term, hence the term wMAPE. \\( \\frac{y(t)}{s} \\cdot pe(t) = \\frac{y(t)}{s} \\cdot \\frac{y(t)-f(t)}{y(t)} \\) - MAPE is sentitive to very small changes in low volume data. However, wMAPE is less sensitive to it. - MAPE assumes that, absolute error on each item is equally important. Large error on a low-value item can unfairly skew the overall error. In contrast, wMAPE penalizes more for errors in high volume data as compared to low volume data. - wMAPE do not have divide by zero error. references MASE # Mean Absolute Scaled Error\nMASE \\( \\text{MASE}=\\text{mean}\\left(|se(t)|\\right),\\text{where } s = \\frac{1}{n-1}\\sum_{i=2}^{n} |y(t)-y(t-1)| \\) notes - MASE is independent of the scale of the data. - The scale parameter $s$ is the *in-sample* MAE from the naive forecast method. \\( s = \\frac{1}{n-1}\\sum_{i=1}^{n} |y(t)-y(t-1)| \\) - A scaled error is less than one if it arises from a better forecast than the average one-step, naive forecast computed in-sample. - The only scenario under which MASE would be infinite or undefined is when all historical observations are equal. references - Rob J. Hyndman and Anne B. Koehler, [Another look at measures of forecast accuracy](https://github.ibm.com/retail-supply-chain/planning/files/582845/Another.look.at.measures.of.forecast.accuracy.pdf), International Journal of Forecasting, Volume 22, Issue 4, 2006. - Rob J Hyndman, [Another look at forecast-accuracy metrics for intermittent demand](https://github.ibm.com/retail-supply-chain/planning/files/582847/Metrics.for.intermitted.demand.pdf). Chapter 3.4, pages 204-211, in \"Business Forecasting: Practical Problems and Solutions\", John Wiley \u0026 Sons, 2015. RMSSE # Root Mean Squared Scaled Error\nRMSSE \\( \\text{RMSSE}=\\sqrt{\\text{mean}\\left(se(t)^2\\right)},\\text{where } s = \\frac{1}{n-1}\\sum_{i=2}^{n} |y(t)-y(t-1)| \\) notes - This is the root mean squared variant of MASE. - This is suited for time series characterized by intermittency, involving spardic unit sales with lots of zeros. - Absolute errors used by MASE are optimized for the median and would assign lower scores to forecasting methods that derive the forecasts close to zero. RMSSE buils on squared errors, which are optimized for the mean. - The measure is scale independent, meaning that it can be effectively used to compare forecasts across series with different scales. - The measure penalizes positive and negative forecast errors, as well as large and small forecasts, equally, thus being symmetric. - It can be safely computed as it does not rely on divisions with values that could be equal or close to zero (e.g. as done in percentage errors when $y(t)=0$ or relative errors when the error of the benchmark used for scaling is zero). references - [The M5 Competition](https://mofc.unic.ac.cy/m5-competition/) - The M5 series are characterized by intermittency, involving sporadic unit sales with lots of zeros. This means that absolute errors, which are optimized for the median, would assign lower scores (better performance) to forecasting methods that derive forecasts close to zero. However, the objective of M5 is to accurately forecast the average demand and for this reason, the accuracy measure used builds on squared errors, which are optimized for the mean. MSLE # Mean Squared Log Error\nMSLE \\( \\text{MSLE}= \\text{mean} \\left(\\log(y(t)+1)- \\log(f(t)+1)\\right) ^2 \\) notes - The introduction of the logarithm makes MSLE only care about the relative difference between the true and the predicted value. Thus making this scale independent. - Robustness: In the case of MSE, the presence of outliers can explode the error term to a very high value. But, in the case of MLSE the outliers are drastically scaled down therefore nullifying their effect. - Biased penalty: MSLE penalizes underestimates more than overestimates because of an asymmetry in the error curve. - The reason ‘1’ is added to both $y(t)$ and $f(t)$ is for mathematical convenience since $\\log(0)$ is not defined but both $y(t)$ or $f(t)$ can be 0. references Kim, Sungil, and Heeyoung Kim. [A new metric of absolute percentage error for intermittent demand forecasts](https://core.ac.uk/reader/82178886). International Journal of Forecasting 32.3 (2016): 669-679. Correlation based metrics # PCC # Pearson correlation coefficient ($r$)\nPCC \\( r = \\frac{\\sum_{t = n+1}^{n+h} (y(t) - \\text{mean} (y(t)))(f(t) - \\text{mean} (f(t)))}{\\sqrt{\\sum_{t = n+1}^{n+h} (y(t) - \\text{mean} (y(t)))^2}\\sqrt{\\sum_{t = n+1}^{n+h} (f(t) - \\text{mean} (f(t)))^2}} \\) notes - Pearson correlation measures how two continuous signals co-vary over time and indicate the linear relationship as a number between -1 (negatively correlated) to 0 (not correlated) to 1 (perfectly correlated). - Sensitive to outliers. - Based on the assumption of homoscedasticity of the data (variance of your data is homogenous across the data range). - It is a scale independent and offset independent metric. - Caution is necessary when using Pearson's correlation coefficient for model selection (see references). references - Waldmann, Patrik. [On the use of the Pearson correlation coefficient for model evaluation in genome-wide prediction](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6781837/) Frontiers in Genetics 10 (2019): 899. - Armstrong, Richard A. [Should Pearson's correlation coefficient be avoided?.](https://onlinelibrary.wiley.com/doi/full/10.1111/opo.12636) Ophthalmic and Physiological Optics 39.5 (2019): 316-327. KRCC # Kendall rank correlation coefficient ($\\tau $)\nA rank correlation coefficient measures the degree of similarity between two rankings, and can be used to assess the significance of the relation between them.\nFor $t_i \u0026lt; t_j$, two sequences $y(t)$ and $f(t)$ are said to be concordant if the ranks for both elements agree: that is, if both $f(t_i)\u0026gt;f(t_j)$ and $y(t_i)\u0026gt;y(t_j)$; or if both $f(t_i)\u0026lt;f(t_j)$ and $y(t_i)\u0026lt;y(t_j)$.\nThey are said to be discordant, if $fx(t_i)\u0026gt;f(t_j)$ and $y(t_i)\u0026lt;y(t_j)$ or if $f(t_i) \u0026lt; f(t_j)$ and $y(t_i)\u0026gt;y(t_j) $.\nIf $x(t_i)=x(t_j)$ and $y(t_i)=y(t_j) $, the pair is neither concordant nor discordant.\nKRCC \\( \\tau = \\frac{(\\text{number of concordant pairs}) - (\\text{number of discordant pairs})}{ {n \\choose 2} } \\) notes - The denominator ${n \\choose 2}$ is the total number of pair combinations, so the coefficient must be in the range $−1 \\leq \\tau \\leq 1$. - If the agreement between the two rankings is perfect (i.e., the two rankings are the same) the coefficient has value 1. - If the disagreement between the two rankings is perfect (i.e., one ranking is the reverse of the other) the coefficient has value −1. - If $X$ and $Y$ are independent, then we would expect the coefficient to be approximately zero. - An explicit expression for Kendall's rank coefficient is $\\tau= \\frac{2}{n(n-1)}\\sum_{i"}]