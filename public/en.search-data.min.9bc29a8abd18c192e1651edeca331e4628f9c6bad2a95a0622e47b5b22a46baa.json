[{"id":0,"href":"/docs/training/","title":"Training deep neural networks","section":"Docs","content":"The goal of training is to find the value of the parameters of a model to make effective predictions.\nWe choose the model parameters by optimizing a loss funcntion.\n"},{"id":1,"href":"/docs/training/model/","title":"Models","section":"Training deep neural networks","content":" Single Layer Networks # For simplicity for this chapter we will mainly introduce single layer networks for regression and classification.\nstateDiagram-v2 direction LR z1: $$x_1$$ z2: $$x_2$$ zi: $$x_i$$ zM: $$x_d$$ aj: $$a=\\sum_i w_{i} x_i$$ zj: $$z=h(a)$$ z1 --\u003e aj:$$w_{1}$$ z2 --\u003e aj:$$w_{2}$$ zi --\u003e aj:$$w_{i}$$ zM --\u003e aj:$$w_{d}$$ aj --\u003e zj zj --\u003e END:::hidden note left of zM : Inputs note left of aj : Pre-activation note left of zj : Activation note left of END : Output classDef hidden display: none; Linear Regression # Linear Regression is a single layer neural network for regression. The probability of \\(y\\) for a given feature vector ( \\(\\mathbf{x}\\in \\mathbb{R}^d\\) ) is modelled as \\[ \\text{Pr}[y|\\mathbf{x},\\mathbf{w}] = \\mathcal{N}(y|\\mathbf{w}^T\\mathbf{x},\\sigma^2) \\] where \\(\\mathbf{w}\\in \\mathbb{R}^d\\) are the weights/parameters of the model and \\(\\mathcal{N}\\) is the normal distribution with mean \\(\\mathbf{w}^T\\mathbf{x}\\) and variance \\(\\sigma^2\\) . The prediction is given by \\[ \\text{E}[y|\\mathbf{x},\\mathbf{w}] = \\mathbf{w}^T\\mathbf{x} \\] Without loss of generalization we ignore the bias term as it can be incorporated into the feature vector.\nGiven a dataset \\(\\mathcal{D}=\\{\\mathbf{x}_i \\in \\mathbb{R}^d,\\mathbf{y}_i \\in \\mathbb{R}\\}_{i=1}^N\\) containing \\(n\\) examples we need to estimate the parameter vector \\(\\mathbf{w}\\) by maximizing the likelihood of data.\nIn practice we minimize the negative log likelihood.\nLet \\( \\mu_i = \\mathbf{w}^T\\mathbf{x}_i\\) be the model prediction for each example in the training dataset. The negative log likelihood (NLL) is given by \\[ \\begin{align} L(\\mathbf{w}) \u0026= - \\sum_{i=1}^{N} \\log \\left[\\text{Pr}[y_i|\\mathbf{x}_i,\\mathbf{w}]\\right] \\nonumber \\\\ \u0026= \\frac{N}{2} \\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i-\\mu_i)^2 \\nonumber \\\\ \\end{align} \\] This is equivalent to minimizing the Mean Squared Error (MSE) loss. \\[ \\begin{align} L(\\mathbf{w}) \u0026= \\frac{1}{N} \\sum_{i=1}^{N} (y_i-\\mu_i)^2 \\nonumber \\\\ \\end{align} \\] We need to choose the model parameters that optimizes (minimizes) the loss function. \\[ \\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}} L(\\mathbf{w}) \\] torch.nn.MSELoss Logistic Regression # Logisitc Regression is a single layer neural network for binary classification. The probability of the positive class ( \\(y=1\\) ) for a given feature vector ( \\(\\mathbf{x}\\in \\mathbb{R}^d\\) ) is given by \\[ \\text{Pr}[y=1|\\mathbf{x},\\mathbf{w}] = \\sigma(\\mathbf{w}^T\\mathbf{x}) \\] where \\(\\mathbf{w}\\in \\mathbb{R}^d\\) are the weights/parameters of the model and \\(\\sigma\\) is the sigmoid activation function defined as \\[ \\sigma(x) = \\frac{1}{1-e^{-z}} \\] Without loss of generalization we ignore the bias term as it can be incorporated into the feature vector.\nGiven a dataset \\(\\mathcal{D}=\\{\\mathbf{x}_i \\in \\mathbb{R}^d,\\mathbf{y}_i \\in [0,1]\\}_{i=1}^N\\) containing \\(n\\) examples we need to estimate the parameter vector \\(\\mathbf{w}\\) by maximizing the likelihood of data.\nIn practice we minimize the negative log likelihood.\nLet \\( \\mu_i = \\text{Pr}[y_i=1|\\mathbf{x}_i,\\mathbf{w}] = \\sigma(\\mathbf{w}^T\\mathbf{x}_i)\\) be the model prediction for each example in the training dataset. The the negative log likelihood (NLL) is given by \\[ \\begin{align} L(\\mathbf{w}) \u0026= - \\sum_{i=1}^{N} \\log\\left[\\mu_i^{y_i}(1-\\mu_i)^{1-y_i}\\right] \\nonumber \\\\ \u0026= - \\sum_{i=1}^{N} \\left[ y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i) \\right] \\nonumber \\\\ \\end{align} \\] This is referred to as the Binary Cross Entropy (BCE) loss. We need to choose the model parameters that optimizes (minimizes) the loss function. \\[ \\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}} L(\\mathbf{w}) \\] torch.nn.BCELoss torch.nn.BCEWithLogitsLoss Entropy \\(\\) The entropy of a discrete random variable \\(X\\) with \\(K\\) states/categories with distribution \\(p_k = \\text{Pr}(X=k)\\) for \\(k=1,...,K\\) is a measure of uncertainty and is defined as follows. \\[H(X) = \\sum_{k=1}^{K} p_k \\log_2 \\frac{1}{p_k} = - \\sum_{k=1}^{K} p_k \\log_2 p_k \\] \\(\\) The term \\(\\log_2\\frac{1}{p}\\) quantifies the notion or surprise or uncertainty and entropy is the average uncertainty. The unit is bits ( \\(\\in [0,\\log_2 K]\\) ) (or nats incase of natural log). The discrete distribution with maximum entropy ( \\(\\log_2 K\\) ) is uniform. The discrete distribution with minimum entropy ( \\(0\\) ) is any delta function which puts all mass on one state/category. Binary entropy\n\\(\\) For a binary random variable \\(X \\in {0,1}\\) with \\(\\text{Pr}(X=1) = \\theta\\) and \\(\\text{Pr}(X=0) = 1-\\theta\\) the entropy is as follows. \\[H(\\theta) = - [ \\theta \\log_2 \\theta + (1-\\theta) \\log_2 (1-\\theta) ] \\] \\[H(\\theta) \\in [0,1]\\] and is maximum when \\(\\theta=0.5\\) . Cross entropy\n\\(\\) Cross entropy is the average number of bits needed to encode the data from from a source \\(p\\) when we model it using \\(q\\) . \\[H(p,q) = - \\sum_{k=1}^{K} p_k \\log_2 q_k \\] Loss functions # https://pytorch.org/docs/stable/nn.html#loss-functions\n"},{"id":2,"href":"/docs/training/gradient_descent/","title":"Gradient Descent","section":"Training deep neural networks","content":" Gradient Descent # Steepest descent.\nLet \\(\\mathbf{w}\\) be a vector of all the parameters for a model.\nLet \\(L(\\mathbf{w})\\) be the loss function (or error function).\nWe need to choose the model parameters that optimizes (minimizes) the loss function.\n\\[ \\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}} L(\\mathbf{w}) \\] Let \\(\\nabla L(\\mathbf{w})\\) be the gradient vector, where each element is the partial derivative of the loss fucntion wrt each parameter.\nThe gradient vector points in the direction of the greatest rate of increase of the loss function.\nSo to mimimize the loss function we take small steps in the direction of \\(-\\nabla L(\\mathbf{w})\\) .\nAt the mimimum \\(\\nabla L(\\mathbf{w})=0\\) .\n\\(\\nabla L(\\mathbf{w})=0\\) .\nStationary points \\(\\nabla L(\\mathbf{w})=0\\) are knows as stationary points, which can be either a minima, maxima or a saddle point. The necessary and sufficient condition for a local minima is The gradient of the loss function should be zero. The Hessian matrix should be positive definite. For now we will assume the gradient is given. For deep neural networks the gradient can be computed efficiently via backpropagation (which we will revisit later).\nBatch Gradient Descent # We take a small step in the direction of the negative gradient.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L(\\mathbf{w}^{t-1}) \\] The parameter \\(\\eta \u003e 0\\) is called the learning rate and determines the step size at each iteration.\nThis update is repeated multiple times (till covergence).\nfor epoch in range(n_epochs): dw = gradient(loss, data, w) w = w - lr * dw Each step requires that the entire training data be processed to compute the gradient \\(\\nabla L(\\mathbf{w}^{t-1})\\) . For large datasets this is not comptationally efficient.\nStochastic Gradient Descent # In general most loss functions can be written as sum over each training instance. \\[ L(\\mathbf{w}) = \\sum_{i=1}^{N} L_i(\\mathbf{w}) \\] In Stochastic Gradient Descent (SGD) we update the parameters one data point at a time. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L_i(\\mathbf{w}^{t-1}) \\] A complete passthrough of the whole dataset is called an epoch. Training is done for multiple epochs depending on the size of the dataset.\nfor epoch in range(n_epochs): for i in range(n_data): dw = gradient(loss, data[i], w) w = w - lr * dw SGD is much faster and more computationally efficient, but it has noise in the estimation of the gradient. Since it updates the weight frequently, it can lead to big oscillations and that makes the training process highly unstable. Bottou, L. (2010). Large-Scale Machine Learning with Stochastic Gradient Descent. In: Lechevallier, Y., Saporta, G. (eds) Proceedings of COMPSTAT'2010. Physica-Verlag HD.\nMini-batch Stochastic Gradient Descent # Using a single example results in a very noisy estimate of the gradient. So we use a small random subset of data called mini-batch of size B (batch size) to compute the gradient.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L_{batch}(\\mathbf{w}^{t-1}) \\] for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) w = w - lr * dw PyTorch optimizer = optim.SGD(model.parameters(), lr=1e-3) Mini-batch SGD is the most commonly used method and is sometimes refered to as just SGD.\nTypical choices of the batch size are B=32,64,128,256,.. In practice we do a random shuffle of the data per epoch. In practice, mini-batch SGD is the most frequently used variation because it is both computationally cheap and results in more robust convergence.\nAdding momentum # One of the basic improvements over SGD comes from adding a momentum term.\nAt every time step, we update velocity by decaying the previous velocity by a factor of \\(0 \\leq \\mu \\leq 1\\) (called the momentum parameter) and adding the current gradient update. \\[ \\mathbf{v}^{t-1} \\leftarrow \\mu \\mathbf{v}^{t-2} - \\eta \\nabla L(\\mathbf{w}^{t-1}) \\] Then, we update our weights in the direction of the velocity vector. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} + \\mathbf{v}^{t-1} \\] for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient v = momentum * v - lr * dw # velocity w = w + v PyTorch optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) We now have two hyper-parameters learnign rate and momentum. Typically we set the momentum parameter to 0.9.\nEffective learning rate \\(\\) One interpretation of momentum to increase the effective learning rate from \\(\\eta\\) to \\(\\frac{\\eta}{(1-\\mu)}\\) . If we make the approximation that the gradient is unchanging then \\[ -\\eta \\nabla L \\{1+\\mu+\\mu^2+...\\} = - \\frac{\\eta}{1-\\mu} \\nabla L \\] By contrast, in a region of high curvature in which gradient descent is oscillatory, successive contributions from the momentum term will tend to cancel and effective learning rate will be close to \\(\\eta\\) . We can now escape local minima or saddle points because we keep moving downwards even though the gradient of the mini-batch might be zero. Momentum can also help us reduce the oscillation of the gradients because the velocity vectors can smooth out these highly changing landscapes. It reduces the noise of the gradients and follows a more direct walk down the landscape. Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. 2013. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28 (ICML'13). JMLR.org, III–1139–III–1147.\nAdaptive Learning Rates # Different learning rate for each parameter.\nAdagrad # Adaptive gradient.\nAdaGrad reduces each learning rate parameter over time by using the accumulated sum of squares of all the derivates calculated for that parameter. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{r}^{t}}+\\delta} \\odot \\nabla L(\\mathbf{w}^{t-1}) \\] where \\(\\mathbf{r}^t\\) is the running sum of the squares of the gradients and \\(\\delta\\) is a small constant to ensure numerical stability. \\[ \\mathbf{r}^t = \\mathbf{r}^{t-1} + \\left(\\nabla L(\\mathbf{w}^{t})\\right)^2 \\] for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient r += dw*dw # Accumulated squared gradients w = w - lr * dw / (r.sqrt() + delta) PyTorch optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, eps=1e-10) We can see that when the gradient is changing very fast, the learning rate will be smaller. When the gradient is changing slowly, the learning rate will be bigger.\nA drawback of Adagrad is that as time goes by, the learning rate becomes smaller and smaller due to the monotonic increment of the running squared sum.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. J. Mach. Learn. Res. 12, null (2/1/2011), 2121–2159.\nRMSProp # Root Mean Square Propagation, Leaky AdaGrad\nSince AdaGrad accumulates the squared gradients from the beginning, the associatied weight updates can become very small as training progresses.\nRMSProp essentially replaces it with an exponentialy weighted average. \\[ \\mathbf{r}^t = \\alpha \\mathbf{r}^{t-1} + (1-\\alpha) \\left(\\nabla L(\\mathbf{w}^{t})\\right)^2 \\] where \\(0 \u003c \\alpha \u003c 1\\) . \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{r}^{t}}+\\delta} \\odot \\nabla L(\\mathbf{w}^{t-1}) \\] \\(\\) Typically we set the \\(\\alpha=0.9\\) . for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient r += alpha * r + (1-alpha) * dw*dw # Accumulated squared gradients w = w - lr * dw / (r.sqrt() + delta) PyTorch optimizer = torch.optim.RMSProp(model.parameters(), lr=0.01, alpha=0.99, eps=1e-8) Hinton, 2012. Neural Networks for Machine Learning. Lecture 6a.\nAdam # Adaptive moments.\nIf we combine RMSProp with momentum we ontain the most popular Adam optimization method.\nAdam maintains an exponentially weighted average of the first and the second moments. \\[ \\mathbf{s}^t = \\beta_1 \\mathbf{s}^{t-1} + (1-\\beta_1) \\left(\\nabla L(\\mathbf{w}^{t})\\right) \\] \\[ \\mathbf{r}^t = \\beta_2 \\mathbf{r}^{t-1} + (1-\\beta_2) \\left(\\nabla L(\\mathbf{w}^{t})\\right)^2 \\] We correct for the bias introduced by initializing \\(\\mathbf{s}^0\\) and \\(\\mathbf{r}^0\\) to zero. \\[ \\hat{\\mathbf{s}}^t = \\frac{\\mathbf{s}^t}{1-\\beta_1^t} \\] \\[ \\hat{\\mathbf{r}}^t = \\frac{\\mathbf{r}^t}{1-\\beta_2^t} \\] The updates are given as follows. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{\\mathbf{r}}^{t}}+\\delta} \\odot \\hat{\\mathbf{s}}^t \\] \\(\\) Typically we set the \\(\\beta_1=0.9\\) and \\(\\beta_2=0.99\\) . for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient s += beta1 * s + (1-beta1) * dw # Accumulated gradients r += beta2 * r + (1-beta2) * dw*dw # Accumulated squared gradients s_hat = s /(1-beta1**t) r_hat = r /(1-beta2**t) w = w - lr * s_hat / (r_hat.sqrt() + delta) PyTorch optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.99), eps=1e-08) Kingma, D.P. and Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\nAdam is the most widely used optimizer.\nLearning rate schedule # A small learning rate leads to slow convergence while a large learning rate leads to instability (due to divergent oscillations).\nIn practice we start with a large learning rate and and then reduce itover time.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla^{t-1} L(\\mathbf{w}^{t-1}) \\] Linear \\[ \\mathbf{\\eta}^t = \\left(1-\\frac{t}{K}\\right) \\mathbf{\\eta}^0 + \\left(\\frac{t}{K}\\right) \\mathbf{\\eta}^K \\] The learning rate reduces linearly over K steps, after which its value is held constant.\nPower \\[ \\mathbf{\\eta}^t = \\mathbf{\\eta}^0 \\left(1+\\frac{t}{s}\\right)^c \\] Exponential \\[ \\mathbf{\\eta}^t = \\mathbf{\\eta}^0 c^\\frac{t}{s}\\] from torch.optim import SGD from torch.optim.lr_scheduler import ExponentialLR optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9) scheduler = ExponentialLR(optimizer, gamma=0.9) PyTorch Learning Curve # Track the learning curve.\nParameter initialization # Initialization before starting the gradient descent.\nAvoid all parameters set to same value. (symmetry breaking)\nUniform distribution in the range \\([-\\epsilon,\\epsilon]\\) Zero-mean Gaussian \\(\\mathcal{N}(0,\\epsilon^2)\\) nn.init Collateral # https://pytorch.org/docs/stable/optim.html "},{"id":3,"href":"/docs/training/backpropagation/","title":"Backpropagation","section":"Training deep neural networks","content":" Backpropagation # Backprop, Error Backpropagation.\nBackpropagation (or backprop) is an efficient technique to compute the gradient of the loss function.\nIt boils down to a local message passing scheme in which information is sent backwards through the network.\nForward propagation # stateDiagram-v2 direction LR z1: $$z_1$$ z2: $$z_2$$ zi: $$z_i$$ zM: $$...$$ aj: $$a_j=\\sum_i w_{ji} z_i$$ zj: $$z_j=h(a_j)$$ START1:::hidden --\u003e z1 START2:::hidden --\u003e z2 STARTi:::hidden --\u003e zi STARTM:::hidden --\u003e zM z1 --\u003e aj z2 --\u003e aj zi --\u003e aj:$$w_{ji}$$ zM --\u003e aj aj --\u003e zj zj --\u003e END:::hidden note left of aj : Pre-activation note left of zj : Activation classDef hidden display: none; Let\u0026rsquo;s consider a hidden unit in a general feed forward neural nework. \\[ a_j=\\sum_i w_{ji} z_i \\] where \\(z_i\\) is the activation of anoter unit or an input that sends an connection of unit \\(j\\) and \\(w_{ji}\\) is the weight associated with that connection. \\(a_j\\) is known as pre-activation and is transformed by a non-linear activation fucntion to give the activation \\(z_j\\) of unit \\(j\\) . \\[ z_j=h(a_j) \\] For any given data point in the traning set, we can pass the input and compute the activations of all the hidden and output units. This process is called forward propagation since it is the forward flow of information through the network.\nBackward propagation # \\[ \\frac{\\partial L_n}{\\partial w_{ji}} = \\frac{\\partial L_n}{\\partial a_{j}} \\frac{\\partial a_j}{\\partial w_{ji}} = \\delta_j z_i \\] where \\(\\delta_j\\) are referred to as errors \\[ \\frac{\\partial L_n}{\\partial a_{j}} := \\delta_j \\] and \\[ \\frac{\\partial a_j}{\\partial w_{ji}} = z_i \\] So we now have \\[ \\frac{\\partial L_n}{\\partial w_{ji}} = \\delta_j z_i \\] The required derivative is simply obtained by multiplying the value of \\(\\delta\\) for the unit at the output end of the weight by the value of \\(z\\) for the unit at the input end of the weight.\n\\(\\delta\\) for the output units are based on the losss function.\nTo evaluate the \\(\\delta\\) for the hidden units we again make use of the the chain rule for partial derivatives. \\[ \\delta_j := \\frac{\\partial L_n}{\\partial a_{j}} = \\sum_{k} \\frac{\\partial L_n}{\\partial a_{k}} \\frac{\\partial a_k}{\\partial a_{j}} \\] where the sum runs over all the units k to which j sends connections. \\[ \\delta_j = h^{'}(a_j)\\sum_{k} w_{kj} \\delta_k \\] This tells us that the value of \\(\\delta\\) for a particular hidden unit can be obtained by propagating the \\(\\delta\\) backward from uits higher up in the network.\nstateDiagram-v2 direction LR z1: $$z_1$$ z2: $$z_2$$ zi: $$z_i$$ zM: $$...$$ delta1: $$\\delta_1$$ delta2: $$\\delta_2$$ deltak: $$\\delta_k$$ deltaM: $$...$$ aj: $$a_j$$ zj: $$z_j$$ START1:::hidden --\u003e z1 START2:::hidden --\u003e z2 STARTi:::hidden --\u003e zi STARTM:::hidden --\u003e zM z1 --\u003e aj z2 --\u003e aj zi --\u003e aj:$$w_{ji}$$ zM --\u003e aj aj --\u003e zj zj --\u003e delta1 zj --\u003e delta2 zj --\u003e deltak:$$w_{kj}$$ zj --\u003e deltaM delta1 --\u003e zj delta2 --\u003e zj deltak --\u003e zj deltaM --\u003e zj delta1 --\u003e START11:::hidden delta2 --\u003e START22:::hidden deltak --\u003e STARTii:::hidden deltaM --\u003e STARTMM:::hidden note left of aj : Pre-activation note left of zj : Activation note right of deltak : Errors classDef hidden display: none; Forward propagation # For all hidden and ouput units compute in forward order\n\\[ a_j \\leftarrow \\sum_i w_{ji} z_i \\] \\[ z_j \\leftarrow h(a_j) \\] Error evaluation # For all output units compute\n\\[ \\delta_k \\leftarrow \\frac{\\partial L_n}{\\partial a_k} \\] Backward propagation # For all hidden units compute in reverse order\n\\[ \\delta_j \\leftarrow h^{'}(a_j)\\sum_{k} w_{kj} \\delta_k \\] \\[ \\frac{\\partial L_n}{\\partial w_{ji}} \\leftarrow \\delta_j z_i \\] Automatic differenciation # Algorithmic differentiation, autodiff, autograd\nThere are broadly 4 appoaches to compute derivatives.\nApproach Pros Cons Manual derivation of backprop equations. If done carefully can result in efficent code. Manual process, prone to erros and not easy to iterate on models Numerical evaluation of gradients via finite differences. Sometime sused to check for correctness of other methods. Limited by computational accuracy. Scales poorly with the size of the network. Symbolic differenciation using packages like sympy Closed form needed. Resulting expression can be very long (expression swell). Automatic differentiation Most prefered. Atılım Günes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. Automatic differentiation in machine learning: a survey. J. Mach. Learn. Res. 18, 1 (January 2017), 5595–5637.\nForward-mode automatic differentiation # We augment each intermediate variable \\(z_i\\) (known as primal variable) with an additional variable representing the value of some derivative of that variable, which we denote as \\(\\dot{z}_i\\) , known as tangent variable. The tangent variables are generated automatically.\nConsider the following function. \\[ f(x_1,x_2) = x_1x_2 + \\exp(x_1x_2) - \\sin(x_2) \\] When implemented in software the code consists of a sequence of operations than can be expressed as an evaluation trace of the underlying elementary operations. This trace can be visualized as a computation graph with respect to the following 7 primal variables. stateDiagram-v2 direction LR x1: $$x_1$$ x2: $$x_2$$ v1: $$v_1 = x_1$$ v2: $$v_2 = x_2$$ v3: $$v_3 = v_1v_2$$ v4: $$v_4 = \\sin(v_2)$$ v5: $$v_5 = \\exp(v_3)$$ v6: $$v_6 = v_3 - v_4$$ v7: $$v_7 = v_5 + v_6$$ f: $$f = v_5 + v_6$$ x1 --\u003e v1 x2 --\u003e v2 v1 --\u003e v3 v2 --\u003e v4 v2 --\u003e v3 v3 --\u003e v5 v4 --\u003e v6 v3 --\u003e v6 v5 --\u003e v7 v6 --\u003e v7 v7 --\u003e f We first write code to implement the evaluation of the primal variables. \\[ v_1 = x_1 \\] \\[ v_2 = x_2 \\] \\[ v_3 = v_1v_2 \\] \\[ v_4 = \\sin(v_2) \\] \\[ v_5 = \\exp(v_3) \\] \\[ v_6 = v_3 - v_4 \\] \\[ v7 = v_5 + v_6 \\] Not say we wish to evaluate the derivative \\(\\partial f/\\partial x_1\\) . First we define the tangent variables by \\[\\dot{v}_i = \\frac{\\partial v_i}{\\partial x_1}\\] Expressions for evaluating these can be constructed automatically using the chain rule of calculus. \\[ \\dot{v}_i = \\frac{\\partial v_i}{\\partial x_1} = \\sum_{j\\in\\text{parents}(i)} \\frac{\\partial v_i}{\\partial v_j} \\frac{\\partial v_j}{\\partial x_1} = \\sum_{j\\in\\text{parents}(i)} \\dot{v}_j \\frac{\\partial v_i}{\\partial v_j} \\] where \\(\\text{parents}(i)\\) denotes the set of parents of node i in the evaluation trace diagram.\nThe associated euqations and correspoding code for evaluating the tangent variables are generated automatically. \\[ \\dot{v}_1 = 1 \\] \\[ \\dot{v}_2 = 0 \\] \\[ \\dot{v}_3 = v_1\\dot{v}_2+\\dot{v}_1v_2 \\] \\[ \\dot{v}_4 = \\dot{v}_2\\cos(v_2) \\] \\[ \\dot{v}_5 = \\dot{v}_3\\exp(v_3) \\] \\[ \\dot{v}_6 = \\dot{v}_3 - \\dot{v}_4 \\] \\[ \\dot{v}_7 = \\dot{v}_5 + \\dot{v}_6 \\] To evaluate the derivative \\(\\frac{\\partial f}{\\partial x_1}\\) we input specific values of \\(x_1\\) and \\(x_2\\) and the code then executes the primal and tangent equations, numerically evalating the tuples \\((v_i,\\dot{v}_i)\\) in forward order untill we obtain the required derivative.\nThe forward mode with slight modifications can handle multiple outputs in the same pass but the proces has to be repeated for every parameter that we need the derivative. Since we are often in the rgime of one output with millions of parameters this is not scalable for modern deep neural networks. We therefore turn to an alternative version based on the backwards flow of derivative data through the evaluation trace graph.\nReverse-mode automatic differentiation # Reverse-mode automatic differentiation is a gernalization of the error backpropagation procedure we discussed earlier.\nAs with forward mode, we augment each primal variable \\(v_i\\) with an additional variable called adjoint variable, denoted as \\(\\bar{v}_i\\) . \\[\\bar{v}_i = \\frac{\\partial f}{\\partial v_i}\\] Expressions for evaluating these can be constructed automatically using the chain rule of calculus. \\[ \\bar{v}_i = \\frac{\\partial f}{\\partial v_i} = \\sum_{j\\in\\text{children}(i)} \\frac{\\partial f}{\\partial v_j} \\frac{\\partial v_j}{\\partial v_i} = \\sum_{j\\in\\text{children}(i)} \\bar{v}_j \\frac{\\partial v_j}{\\partial v_i} \\] where \\(\\text{children}(i)\\) denotes the set of children of node i in the evaluation trace diagram.\nThe successive evaluation of the adjoint variables represents a flow of information backwards through the graph. For multiple parameters a single backward pass is enough. Reverse mode is more memory intensive than forward mode.\n\\[ \\bar{v}_7 = 1 \\] \\[ \\bar{v}_6 = \\bar{v}_7 \\] \\[ \\bar{v}_5 = \\bar{v}_7 \\] \\[ \\bar{v}_4 = -\\bar{v}_6 \\] \\[ \\bar{v}_3 = \\bar{v}_5v_5+\\bar{v}_6 \\] \\[ \\bar{v}_2 = \\bar{v}_2v_1+\\bar{v}_4\\cos(v_2) \\] \\[ \\bar{v}_1 = \\bar{v}_3v_2 \\] Autograd in pytorch # A Gentle Introduction to torch.autograd The Fundamentals of Autograd "},{"id":4,"href":"/docs/training/normalization/","title":"Normalization","section":"Training deep neural networks","content":" Batch normalization # In batch normalization the mean and variance are computed across the mini-batch separately for each feature/hidden unit. For a mini-batch of size B \\[ \\mu_i = \\frac{1}{B} \\sum_{n=1}^{B} a_{ni} \\] \\[ \\sigma_i^2 = \\frac{1}{B} \\sum_{n=1}^{B} (a_{ni}-\\mu_i)^2 \\] We normalize the pre-activations as follows. \\[ \\hat{a}_{ni} = \\frac{a_{ni}-\\mu_i}{\\sqrt{\\sigma_i^2+\\delta}} \\] \\[ \\tilde{a}_{ni} = \\gamma_i \\hat{a}_{ni} + \\beta_i \\] PyTorch m = nn.BatchNorm1d(num_features) Layer normalization # In layer normalization the mean and variance are computed across the feature/hidden unit for each example seprately. \\[ \\mu_n = \\frac{1}{M} \\sum_{i=1}^{M} a_{ni} \\] \\[ \\sigma_n^2 = \\frac{1}{M} \\sum_{i=1}^{M} (a_{ni}-\\mu_i)^2 \\] We normalize the pre-activations as follows. \\[ \\hat{a}_{ni} = \\frac{a_{ni}-\\mu_n}{\\sqrt{\\sigma_n^2+\\delta}} \\] \\[ \\tilde{a}_{ni} = \\gamma_n \\hat{a}_{ni} + \\beta_n \\] PyTorch layer_norm = nn.LayerNorm(enormalized_shape) Collateral # https://pytorch.org/docs/stable/nn.html#normalization-layers\n"},{"id":5,"href":"/docs/training/regularization/","title":"Regularization","section":"Training deep neural networks","content":" Dropout # Early stopping # "},{"id":6,"href":"/docs/training/training_loop/","title":"Training loop","section":"Training deep neural networks","content":" Training loop # from torch.optim import SGD from torch.optim.lr_scheduler import ExponentialLR optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9) scheduler = ExponentialLR(optimizer, gamma=0.9) for epoch in range(n_epochs): for input, target in dataset: optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step() scheduler.step() "},{"id":7,"href":"/docs/training/quiz/","title":"Quiz","section":"Training deep neural networks","content":" What is the most widely used optimizer ? What are the typically used parameters of the optimizer ? For SGD with momemtum show that it increases the effective learning rate from \\(\\eta\\) to \\(\\frac{\\eta}{(1-\\mu)}\\) . In Attention Is All You Need paper what is the optimizer and the learning rate scheduler used. "},{"id":8,"href":"/docs/training/coding/","title":"Coding","section":"Training deep neural networks","content":" Coding assignment # \\(\\) Setup # https://github.com/vikasraykar/deeplearning-dojo/\ngit clone https://github.com/vikasraykar/deeplearning-dojo.git cd deeplearning-dojo python -m venv .env source .env/bin/activate pip install -r requirements.txt Problem 1 # In the first coding assigment you will be implementing a basic Linear Regression model from scratch using only numpy. You will be implementing a basic batch gradient descent optimizer.\nYou can use only numpy and are not allowed to to use torch or any other python libraries.\nReview the linear regression model and its loss function. Given a feature matrix \\(\\mathbf{X}\\) as a \\(N \\times d\\) numpy.ndarray write the prediction and the loss function using matrix notation and carefully check for dimensions. Accont for the bias by appending the feature matrix with a column of ones. Derive the expression for the gradient of the loss function. Implement a basic batch gradient descent optimizer with a fixed learning rate first. Track the loss every few epochs and check the actual and estimated parameters. Check the MSE loss on the train and the test set. Implement a simple learning rate decay as follows lr=lr/(1+decay_factor*epoch). A sample stub is provided in the repo as below. Your task is to implement the predict and the fit function.\nLinearRegressionNumpy.py \u0026#34;\u0026#34;\u0026#34;Basic implementation of Linear Regression using only numpy. \u0026#34;\u0026#34;\u0026#34; import numpy as np class LinearRegression: \u0026#34;\u0026#34;\u0026#34;Linear Regression.\u0026#34;\u0026#34;\u0026#34; def __init__(self): pass def pred(self, X: np.ndarray) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34;Predction. Args: X (np.ndarray): Features matrix. (N,d) add_colum_vector_for_bias (bool, optional): Add a column vector of ones to model the bias term. Defaults to True. Returns: y (np.ndarray): Prediction vector. (N,) \u0026#34;\u0026#34;\u0026#34; pass def fit( self, X_train: np.ndarray, y_train: np.ndarray, learning_rate: float = 1e-3, learning_rate_decay: bool = True, learning_rate_decay_factor: float = 1.0, num_epochs: int = 100, track_loss_num_epochs: int = 100, ): \u0026#34;\u0026#34;\u0026#34;Training. Args: X_train (np.ndarray): Features matrix. (N,d) y_train (np.ndarray): Target vector. (N,) learning_rate (float, optional): Learning rate. Defaults to 0.001. learning_rate_decay (bool, optional): If True does learning rate deacy. Defaults to True. learning_rate_decay_factor (float, optional): The deacay factor (lr=lr/(1+decay_factor*epoch)). Defaults to 1.0. num_epochs (int, optional): Number of epochs. Defaults to 100. track_loss_num_epochs (int, optional): Compute loss on training set once in k epochs. Defaults to 100. \u0026#34;\u0026#34;\u0026#34; pass numpy # Derive the equation for the gradient of the BCE loss used in logistic regression.\npytorch # Plot learning curve for 3 differnt optimizers.\nAdam implementation # "}]